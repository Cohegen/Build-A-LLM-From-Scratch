{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMtcp255OXBuS4jliDxa/gC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##1.1 Brief Introduction"
      ],
      "metadata": {
        "id": "zhI8eCDXI_3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There are different attention variants here we will discuss: self-attention,causal attention and multi-head attention.\n",
        "* These variants builds on each other, the goal here will be to arrive at a compact and efficient implementation of multi-head attention which we will plug into the LLM architecture."
      ],
      "metadata": {
        "id": "o_7aqaC1JIRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2 Simple-attention with trainable weights"
      ],
      "metadata": {
        "id": "54fPDk1SJ7gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In self-attention our goal is to calculate context vectors for each elements in the input sequence.\n",
        "* Let's use the following input sequence and represent it in a embedding vector.\n"
      ],
      "metadata": {
        "id": "PNY6pDAcKJGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "inputs = torch.tensor(\n",
        "   [ [0.34,0.55,0.66],#Attention(x1)\n",
        "    [0.99,0.87,0.56],#is(x2)\n",
        "    [0.67,0.12,0.65],#all(x3)\n",
        "    [0.99,0.89,0.53],#you(x4)\n",
        "    [0.77,0.67,0.77],#need(x5)\n",
        "   ]\n",
        ")"
      ],
      "metadata": {
        "id": "RW25T2SnKcTo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we initialize the query,key,values weights matrices which willl help us project our input embedding into it's respective query,key and value matrices.\n"
      ],
      "metadata": {
        "id": "0mQfLyskNTGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "w_query = torch.rand(3,5)\n",
        "w_key = torch.rand(3,5)\n",
        "w_value = torch.rand(3,5)"
      ],
      "metadata": {
        "id": "L4Xcni8ONmrc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##computing for our query,key and value matrices\n",
        "query = torch.matmul(inputs, w_query)\n",
        "key = torch.matmul(inputs, w_key)\n",
        "value = torch.matmul(inputs, w_value)\n",
        "print(f\"Query:{query}\")\n",
        "print(f\"Key:{key}\")\n",
        "print(f\"Value:{value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en3tm35JN4Rx",
        "outputId": "8bb39c09-f02f-4587-c160-f9c017f800fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:tensor([[0.7853, 0.7042, 0.1919, 0.4651, 0.6335],\n",
            "        [1.2236, 1.0150, 0.3807, 0.9519, 0.8824],\n",
            "        [0.5073, 0.8091, 0.2301, 0.6112, 0.3424],\n",
            "        [1.2314, 0.9971, 0.3804, 0.9497, 0.8875],\n",
            "        [1.0513, 1.0183, 0.3207, 0.8049, 0.7873]])\n",
            "Key:tensor([[1.0406, 0.5497, 1.1973, 1.2639, 0.4013],\n",
            "        [1.5090, 0.7888, 1.8532, 1.7368, 0.9145],\n",
            "        [0.7995, 0.3310, 1.1880, 0.9585, 0.5008],\n",
            "        [1.5063, 0.7924, 1.8402, 1.7294, 0.9189],\n",
            "        [1.3882, 0.7024, 1.7238, 1.6450, 0.7225]])\n",
            "Value:tensor([[0.8222, 0.4466, 0.7402, 0.9640, 0.9085],\n",
            "        [1.5228, 0.9283, 1.2754, 1.3727, 1.6282],\n",
            "        [0.9374, 0.6549, 0.8192, 1.1236, 0.9768],\n",
            "        [1.5193, 0.9229, 1.2689, 1.3464, 1.6250],\n",
            "        [1.3175, 0.7990, 1.1416, 1.3923, 1.4168]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we calculate the attention scores"
      ],
      "metadata": {
        "id": "XNQM1f-IOwtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = torch.matmul(query,key.T)\n",
        "print(f\"Attention Scores:{attn_scores}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRUjzNRnO2m7",
        "outputId": "0cbd7a0b-b432-427b-bd68-e9b78d91a48d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Scores:tensor([[2.2762, 3.4834, 1.8520, 3.4806, 3.1385],\n",
            "        [3.8442, 5.8128, 3.1209, 5.8050, 5.2712],\n",
            "        [2.1581, 3.2049, 1.7041, 3.2004, 2.9221],\n",
            "        [3.8416, 5.8109, 3.1214, 5.8031, 5.2692],\n",
            "        [3.3711, 5.1021, 2.7245, 5.0962, 4.6205]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we compute the attention weights by using the formula below:\n",
        "    * The formula for the attention weights is given by:\n"
      ],
      "metadata": {
        "id": "_I8vveYHPGlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)$.\n",
        "\n",
        "* Where Q represents query,K represents key, and dk represent output dimension."
      ],
      "metadata": {
        "id": "zqB4TN-CQwv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = key.shape[-1]\n",
        "attn_weights = torch.softmax(attn_scores/d_k**0.5,dim=-1)\n",
        "print(f\"Attention Weights:{attn_weights}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqRNeioqPOL4",
        "outputId": "b69e7d9e-f176-4924-d012-6fa4639058cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:tensor([[0.1486, 0.2551, 0.1230, 0.2547, 0.2186],\n",
            "        [0.1186, 0.2860, 0.0858, 0.2850, 0.2245],\n",
            "        [0.1559, 0.2490, 0.1273, 0.2485, 0.2194],\n",
            "        [0.1186, 0.2860, 0.0859, 0.2850, 0.2245],\n",
            "        [0.1277, 0.2770, 0.0957, 0.2763, 0.2233]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* To get the context vector we do a dot product between attention weights and values matrices"
      ],
      "metadata": {
        "id": "M45Cq8RWRNuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vector = torch.matmul(attn_weights,value)\n",
        "print(f\"Context Vector:{context_vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaSycl-nRZZZ",
        "outputId": "0e266aac-2302-40a0-919e-544aa38e6e7c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector:tensor([[1.3009, 0.7934, 1.1089, 1.2789, 1.3941],\n",
            "        [1.3424, 0.8171, 1.1409, 1.2998, 1.4386],\n",
            "        [1.2932, 0.7887, 1.1030, 1.2750, 1.3859],\n",
            "        [1.3424, 0.8171, 1.1409, 1.2998, 1.4385],\n",
            "        [1.3305, 0.8103, 1.1317, 1.2938, 1.4259]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##implementing a compact self-attention class\n",
        "import torch.nn as nn\n",
        "class SelfAttention_v1(nn.Module):\n",
        "  def __init__(self,d_in,d_out):\n",
        "    super().__init__()\n",
        "    self.w_query = nn.Parameter(torch.rand(d_in,d_out))\n",
        "    self.w_key = nn.Parameter(torch.rand(d_in,d_out))\n",
        "    self.w_value = nn.Parameter(torch.rand(d_in,d_out))\n",
        "\n",
        "  def forward(self,x):\n",
        "    keys = x @ self.w_key\n",
        "    queries = x @ self.w_query\n",
        "    values = x @ self.w_value\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1)\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "rdnRZaMFSq8q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##use case for the class\n",
        "d_in = inputs.shape[-1] # or 3\n",
        "d_out = 5 # As used in previous manual calculations\n",
        "sa_v1 = SelfAttention_v1(d_in,d_out)\n",
        "context_vector = sa_v1(inputs)\n",
        "print(f\"Context Vector:{context_vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENeHqdDtXKE1",
        "outputId": "7c999c14-5323-44dd-d06e-e5d12b91b4b3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector:tensor([[1.1028, 1.0398, 0.9905, 1.1706, 1.2588],\n",
            "        [1.1476, 1.0834, 1.0098, 1.2004, 1.2930],\n",
            "        [1.0974, 1.0344, 0.9883, 1.1669, 1.2544],\n",
            "        [1.1480, 1.0837, 1.0099, 1.2006, 1.2933],\n",
            "        [1.1326, 1.0687, 1.0034, 1.1903, 1.2813]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can improve `SelfAttention_v1` implementation further by utilizing Pytorch's `nn.Linear` layers, which effectively perform matrix multiplications when the bias units are disabled."
      ],
      "metadata": {
        "id": "Fcm3wEsuZMzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## self-attention class using pytorch's\n",
        "class SelfAttention_v2(nn.Module):\n",
        " def __init__(self,d_in,d_out,qkv_bias=False):\n",
        "  super().__init__()\n",
        "  self.w_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "  self.w_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "  self.w_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "\n",
        " def forward(self,x):\n",
        "   keys  = self.w_key(x)\n",
        "   queries = self.w_query(x)\n",
        "   values = self.w_value(x)\n",
        "   attn_scores = queries @ keys.T\n",
        "   attn_weights = torch.softmax(\n",
        "      attn_scores/keys.shape[-1]**0.5,dim=-1\n",
        "  )\n",
        "   context_vec = attn_weights @ values\n",
        "   return context_vec"
      ],
      "metadata": {
        "id": "yC83DImgY9Jy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##use case\n",
        "torch.manual_seed(123)\n",
        "sa_v2 = SelfAttention_v2(d_in,d_out)\n",
        "context_vector = sa_v2(inputs)\n",
        "print(f\"Context Vector:{context_vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf7cm2-TdCd4",
        "outputId": "7df65c64-b5b2-4e60-9251-a02619f34c42"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector:tensor([[ 0.6918,  0.4175, -0.7206, -0.2338,  0.2625],\n",
            "        [ 0.6856,  0.4112, -0.7157, -0.2336,  0.2580],\n",
            "        [ 0.6935,  0.4199, -0.7217, -0.2341,  0.2648],\n",
            "        [ 0.6856,  0.4111, -0.7156, -0.2336,  0.2579],\n",
            "        [ 0.6878,  0.4134, -0.7174, -0.2337,  0.2595]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.3 Causal Attention"
      ],
      "metadata": {
        "id": "7Zw_NGF_d-kN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* For many LLM tasks, you will want the self-attention mechanism to consider only tokens that appear prior to the current position when predicting the next token in a sequence.\n",
        "* Causal attention restricts a model to only consider previous and current inputs in a sequence when processing any given token when computing attention scores."
      ],
      "metadata": {
        "id": "R00kQ-76eCwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##applying a causal attention\n",
        "#1 computing the attention weights using softmax\n",
        "queries = sa_v2.w_query(inputs)\n",
        "keys = sa_v2.w_key(inputs)\n",
        "attn_scores = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5,dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF51GE9YdCgW",
        "outputId": "d54e017f-7fee-4aeb-ef28-522d7a88cc59"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2090, 0.1883, 0.2181, 0.1878, 0.1968],\n",
            "        [0.2184, 0.1807, 0.2265, 0.1803, 0.1941],\n",
            "        [0.2103, 0.1907, 0.2120, 0.1908, 0.1962],\n",
            "        [0.2184, 0.1806, 0.2266, 0.1802, 0.1941],\n",
            "        [0.2151, 0.1834, 0.2238, 0.1830, 0.1948]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#masking values along diagonals\n",
        "context_length = attn_scores.shape[0]\n",
        "mask = torch.tril(torch.ones(context_length,context_length))\n",
        "print(mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuHOlw9dgC_q",
        "outputId": "c80c65f5-b884-46fa-f76c-ba7a453eb5be"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#multiplying mask with attention weights\n",
        "masked = attn_weights*mask\n",
        "print(masked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2K-kIhOgd_6",
        "outputId": "624e2429-18c4-4bd5-94be-aed0febc8d50"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2090, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2184, 0.1807, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2103, 0.1907, 0.2120, 0.0000, 0.0000],\n",
            "        [0.2184, 0.1806, 0.2266, 0.1802, 0.0000],\n",
            "        [0.2151, 0.1834, 0.2238, 0.1830, 0.1948]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## renormalize the attention weights to sum up to to 1\n",
        "row_sums = masked.sum(dim=1,keepdim=True)\n",
        "masked_norm = masked /row_sums\n",
        "print(masked_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTV3aZKLhU_b",
        "outputId": "4db6837a-5a46-4e1e-ace0-118d7f417660"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5472, 0.4528, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3431, 0.3111, 0.3458, 0.0000, 0.0000],\n",
            "        [0.2710, 0.2242, 0.2812, 0.2236, 0.0000],\n",
            "        [0.2151, 0.1834, 0.2238, 0.1830, 0.1948]], grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The softmax function converts its inputs into a probability distribution.\n",
        "* When negative infinity values are present in a row, the softmax function treats them as zero probability."
      ],
      "metadata": {
        "id": "iEliaKsKi2P1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(),-torch.inf)\n",
        "print(masked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17Z_M7mbjyOK",
        "outputId": "bbaa8a04-9dfe-471d-8858-660c8699b997"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.3166,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-0.5420, -0.9652,    -inf,    -inf,    -inf],\n",
            "        [-0.3316, -0.5503, -0.3142,    -inf,    -inf],\n",
            "        [-0.5402, -0.9646, -0.4576, -0.9698,    -inf],\n",
            "        [-0.4812, -0.8374, -0.3923, -0.8418, -0.7029]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##applying softmax\n",
        "attn_weights = torch.softmax(masked,dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHHSmlyqkVwU",
        "outputId": "a6b99a13-114e-4692-ab86-f97ea49777b4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.6042, 0.3958, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3545, 0.2848, 0.3607, 0.0000, 0.0000],\n",
            "        [0.2949, 0.1929, 0.3203, 0.1919, 0.0000],\n",
            "        [0.2330, 0.1632, 0.2547, 0.1625, 0.1867]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    }
  ]
}