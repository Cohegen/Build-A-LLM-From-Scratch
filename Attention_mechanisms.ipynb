{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPV6ft/BPEXoq5OSUHWugiB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##1.1 Brief Introduction"
      ],
      "metadata": {
        "id": "zhI8eCDXI_3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There are different attention variants here we will discuss: self-attention,causal attention and multi-head attention.\n",
        "* These variants builds on each other, the goal here will be to arrive at a compact and efficient implementation of multi-head attention which we will plug into the LLM architecture."
      ],
      "metadata": {
        "id": "o_7aqaC1JIRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2 Simple-attention with trainable weights"
      ],
      "metadata": {
        "id": "54fPDk1SJ7gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In self-attention our goal is to calculate context vectors for each elements in the input sequence.\n",
        "* Let's use the following input sequence and represent it in a embedding vector.\n"
      ],
      "metadata": {
        "id": "PNY6pDAcKJGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "inputs = torch.tensor(\n",
        "   [ [0.34,0.55,0.66],#Attention(x1)\n",
        "    [0.99,0.87,0.56],#is(x2)\n",
        "    [0.67,0.12,0.65],#all(x3)\n",
        "    [0.99,0.89,0.53],#you(x4)\n",
        "    [0.77,0.67,0.77],#need(x5)\n",
        "   ]\n",
        ")"
      ],
      "metadata": {
        "id": "RW25T2SnKcTo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we initialize the query,key,values weights matrices which willl help us project our input embedding into it's respective query,key and value matrices.\n"
      ],
      "metadata": {
        "id": "0mQfLyskNTGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "w_query = torch.rand(3,5)\n",
        "w_key = torch.rand(3,5)\n",
        "w_value = torch.rand(3,5)"
      ],
      "metadata": {
        "id": "L4Xcni8ONmrc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##computing for our query,key and value matrices\n",
        "query = torch.matmul(inputs, w_query)\n",
        "key = torch.matmul(inputs, w_key)\n",
        "value = torch.matmul(inputs, w_value)\n",
        "print(f\"Query:{query}\")\n",
        "print(f\"Key:{key}\")\n",
        "print(f\"Value:{value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en3tm35JN4Rx",
        "outputId": "a6d88788-cfe2-48f4-cd7b-e7a6ff775600"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:tensor([[0.7853, 0.7042, 0.1919, 0.4651, 0.6335],\n",
            "        [1.2236, 1.0150, 0.3807, 0.9519, 0.8824],\n",
            "        [0.5073, 0.8091, 0.2301, 0.6112, 0.3424],\n",
            "        [1.2314, 0.9971, 0.3804, 0.9497, 0.8875],\n",
            "        [1.0513, 1.0183, 0.3207, 0.8049, 0.7873]])\n",
            "Key:tensor([[1.0406, 0.5497, 1.1973, 1.2639, 0.4013],\n",
            "        [1.5090, 0.7888, 1.8532, 1.7368, 0.9145],\n",
            "        [0.7995, 0.3310, 1.1880, 0.9585, 0.5008],\n",
            "        [1.5063, 0.7924, 1.8402, 1.7294, 0.9189],\n",
            "        [1.3882, 0.7024, 1.7238, 1.6450, 0.7225]])\n",
            "Value:tensor([[0.8222, 0.4466, 0.7402, 0.9640, 0.9085],\n",
            "        [1.5228, 0.9283, 1.2754, 1.3727, 1.6282],\n",
            "        [0.9374, 0.6549, 0.8192, 1.1236, 0.9768],\n",
            "        [1.5193, 0.9229, 1.2689, 1.3464, 1.6250],\n",
            "        [1.3175, 0.7990, 1.1416, 1.3923, 1.4168]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we calculate the attention scores"
      ],
      "metadata": {
        "id": "XNQM1f-IOwtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = torch.matmul(query,key.T)\n",
        "print(f\"Attention Scores:{attn_scores}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRUjzNRnO2m7",
        "outputId": "56c2c20e-05fa-48a5-f676-73805a760ae0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Scores:tensor([[2.2762, 3.4834, 1.8520, 3.4806, 3.1385],\n",
            "        [3.8442, 5.8128, 3.1209, 5.8050, 5.2712],\n",
            "        [2.1581, 3.2049, 1.7041, 3.2004, 2.9221],\n",
            "        [3.8416, 5.8109, 3.1214, 5.8031, 5.2692],\n",
            "        [3.3711, 5.1021, 2.7245, 5.0962, 4.6205]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we compute the attention weights by using the formula below:\n",
        "    * The formula for the attention weights is given by:\n"
      ],
      "metadata": {
        "id": "_I8vveYHPGlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)$.\n",
        "\n",
        "* Where Q represents query,K represents key, and dk represent output dimension."
      ],
      "metadata": {
        "id": "zqB4TN-CQwv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = key.shape[-1]\n",
        "attn_weights = torch.softmax(attn_scores/d_k**0.5,dim=-1)\n",
        "print(f\"Attention Weights:{attn_weights}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqRNeioqPOL4",
        "outputId": "4dd238df-5be8-44e4-b5fd-6a76321dc219"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:tensor([[0.1486, 0.2551, 0.1230, 0.2547, 0.2186],\n",
            "        [0.1186, 0.2860, 0.0858, 0.2850, 0.2245],\n",
            "        [0.1559, 0.2490, 0.1273, 0.2485, 0.2194],\n",
            "        [0.1186, 0.2860, 0.0859, 0.2850, 0.2245],\n",
            "        [0.1277, 0.2770, 0.0957, 0.2763, 0.2233]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* To get the context vector we do a dot product between attention weights and values matrices"
      ],
      "metadata": {
        "id": "M45Cq8RWRNuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vector = torch.matmul(attn_weights,value)\n",
        "print(f\"Context Vector:{context_vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaSycl-nRZZZ",
        "outputId": "d77803b9-0777-4116-bdf6-eaf4d1c120fa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector:tensor([[1.3009, 0.7934, 1.1089, 1.2789, 1.3941],\n",
            "        [1.3424, 0.8171, 1.1409, 1.2998, 1.4386],\n",
            "        [1.2932, 0.7887, 1.1030, 1.2750, 1.3859],\n",
            "        [1.3424, 0.8171, 1.1409, 1.2998, 1.4385],\n",
            "        [1.3305, 0.8103, 1.1317, 1.2938, 1.4259]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class SelfAttention_v1(nn.Module):\n",
        "  def __init__(self,d_in,d_out):\n",
        "    super().__init__()\n",
        "    self.w_query = nn.Parameter(torch.rand(d_in,d_out))\n",
        "    self.w_key = nn.Parameter(torch.rand(d_in,d_out))\n",
        "    self.w_value = nn.Parameter(torch.rand(d_in,d_out))"
      ],
      "metadata": {
        "id": "rdnRZaMFSq8q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}