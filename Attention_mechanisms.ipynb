{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOWtbDFBh+ABzqXzl01eFxU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##1.1 Brief Introduction"
      ],
      "metadata": {
        "id": "zhI8eCDXI_3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There are different attention variants here we will discuss: self-attention,causal attention and multi-head attention.\n",
        "* These variants builds on each other, the goal here will be to arrive at a compact and efficient implementation of multi-head attention which we will plug into the LLM architecture."
      ],
      "metadata": {
        "id": "o_7aqaC1JIRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2 Simple-attention with trainable weights"
      ],
      "metadata": {
        "id": "54fPDk1SJ7gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In self-attention our goal is to calculate context vectors for each elements in the input sequence.\n",
        "* Let's use the following input sequence and represent it in a embedding vector.\n"
      ],
      "metadata": {
        "id": "PNY6pDAcKJGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "inputs = torch.tensor(\n",
        "   [ [0.34,0.55,0.66],#Attention(x1)\n",
        "    [0.99,0.87,0.56],#is(x2)\n",
        "    [0.67,0.12,0.65],#all(x3)\n",
        "    [0.99,0.89,0.53],#you(x4)\n",
        "    [0.77,0.67,0.77],#need(x5)\n",
        "   ]\n",
        ")"
      ],
      "metadata": {
        "id": "RW25T2SnKcTo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we initialize the query,key,values weights matrices which willl help us project our input embedding into it's respective query,key and value matrices.\n"
      ],
      "metadata": {
        "id": "0mQfLyskNTGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "w_query = torch.rand(3,5)\n",
        "w_key = torch.rand(3,5)\n",
        "w_value = torch.rand(3,5)"
      ],
      "metadata": {
        "id": "L4Xcni8ONmrc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##computing for our query,key and value matrices\n",
        "query = torch.matmul(inputs, w_query)\n",
        "key = torch.matmul(inputs, w_key)\n",
        "value = torch.matmul(inputs, w_value)\n",
        "print(f\"Query:{query}\")\n",
        "print(f\"Key:{key}\")\n",
        "print(f\"Value:{value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en3tm35JN4Rx",
        "outputId": "81b208d6-fb2a-417c-e659-d2190608bed1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:tensor([[0.7853, 0.7042, 0.1919, 0.4651, 0.6335],\n",
            "        [1.2236, 1.0150, 0.3807, 0.9519, 0.8824],\n",
            "        [0.5073, 0.8091, 0.2301, 0.6112, 0.3424],\n",
            "        [1.2314, 0.9971, 0.3804, 0.9497, 0.8875],\n",
            "        [1.0513, 1.0183, 0.3207, 0.8049, 0.7873]])\n",
            "Key:tensor([[1.0406, 0.5497, 1.1973, 1.2639, 0.4013],\n",
            "        [1.5090, 0.7888, 1.8532, 1.7368, 0.9145],\n",
            "        [0.7995, 0.3310, 1.1880, 0.9585, 0.5008],\n",
            "        [1.5063, 0.7924, 1.8402, 1.7294, 0.9189],\n",
            "        [1.3882, 0.7024, 1.7238, 1.6450, 0.7225]])\n",
            "Value:tensor([[0.8222, 0.4466, 0.7402, 0.9640, 0.9085],\n",
            "        [1.5228, 0.9283, 1.2754, 1.3727, 1.6282],\n",
            "        [0.9374, 0.6549, 0.8192, 1.1236, 0.9768],\n",
            "        [1.5193, 0.9229, 1.2689, 1.3464, 1.6250],\n",
            "        [1.3175, 0.7990, 1.1416, 1.3923, 1.4168]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we calculate the attention scores"
      ],
      "metadata": {
        "id": "XNQM1f-IOwtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = torch.matmul(query,key.T)\n",
        "print(f\"Attention Scores:{attn_scores}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRUjzNRnO2m7",
        "outputId": "a73259e7-bf3e-4ffb-e553-9268bb2a3e05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Scores:tensor([[2.2762, 3.4834, 1.8520, 3.4806, 3.1385],\n",
            "        [3.8442, 5.8128, 3.1209, 5.8050, 5.2712],\n",
            "        [2.1581, 3.2049, 1.7041, 3.2004, 2.9221],\n",
            "        [3.8416, 5.8109, 3.1214, 5.8031, 5.2692],\n",
            "        [3.3711, 5.1021, 2.7245, 5.0962, 4.6205]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we compute the attention weights by using the formula below:\n",
        "    * The formula for the attention weights is given by:\n"
      ],
      "metadata": {
        "id": "_I8vveYHPGlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)$.\n",
        "\n",
        "* Where Q represents query,K represents key, and dk represent output dimension."
      ],
      "metadata": {
        "id": "zqB4TN-CQwv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = key.shape[-1]\n",
        "attn_weights = torch.softmax(attn_scores/d_k**0.5,dim=-1)\n",
        "print(f\"Attention Weights:{attn_weights}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqRNeioqPOL4",
        "outputId": "d880fbbc-d4ed-4f73-c95f-db7ccd1f68ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:tensor([[0.1486, 0.2551, 0.1230, 0.2547, 0.2186],\n",
            "        [0.1186, 0.2860, 0.0858, 0.2850, 0.2245],\n",
            "        [0.1559, 0.2490, 0.1273, 0.2485, 0.2194],\n",
            "        [0.1186, 0.2860, 0.0859, 0.2850, 0.2245],\n",
            "        [0.1277, 0.2770, 0.0957, 0.2763, 0.2233]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* To get the context vector we do a dot product between attention weights and values matrices"
      ],
      "metadata": {
        "id": "M45Cq8RWRNuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vector = torch.matmul(attn_weights,value)\n",
        "print(f\"Context Vector:{context_vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaSycl-nRZZZ",
        "outputId": "ad39858e-219c-4af5-9619-b122a81d9b4b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector:tensor([[1.3009, 0.7934, 1.1089, 1.2789, 1.3941],\n",
            "        [1.3424, 0.8171, 1.1409, 1.2998, 1.4386],\n",
            "        [1.2932, 0.7887, 1.1030, 1.2750, 1.3859],\n",
            "        [1.3424, 0.8171, 1.1409, 1.2998, 1.4385],\n",
            "        [1.3305, 0.8103, 1.1317, 1.2938, 1.4259]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##implementing a compact self-attention class\n",
        "import torch.nn as nn\n",
        "class SelfAttention_v1(nn.Module):\n",
        "  def __init__(self,d_in,d_out):\n",
        "    super().__init__()\n",
        "    self.w_query = nn.Parameter(torch.rand(d_in,d_out))\n",
        "    self.w_key = nn.Parameter(torch.rand(d_in,d_out))\n",
        "    self.w_value = nn.Parameter(torch.rand(d_in,d_out))\n",
        "\n",
        "  def forward(self,x):\n",
        "    keys = x @ self.w_key\n",
        "    queries = x @ self.w_query\n",
        "    values = x @ self.w_value\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1)\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "rdnRZaMFSq8q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##use case for the class\n",
        "d_in = inputs.shape[-1] # or 3\n",
        "d_out = 5 # As used in previous manual calculations\n",
        "sa_v1 = SelfAttention_v1(d_in,d_out)\n",
        "context_vector = sa_v1(inputs)\n",
        "print(f\"Context Vector:{context_vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENeHqdDtXKE1",
        "outputId": "6c770f09-fca8-463e-9efa-226883cc8130"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector:tensor([[1.1028, 1.0398, 0.9905, 1.1706, 1.2588],\n",
            "        [1.1476, 1.0834, 1.0098, 1.2004, 1.2930],\n",
            "        [1.0974, 1.0344, 0.9883, 1.1669, 1.2544],\n",
            "        [1.1480, 1.0837, 1.0099, 1.2006, 1.2933],\n",
            "        [1.1326, 1.0687, 1.0034, 1.1903, 1.2813]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can improve `SelfAttention_v1` implementation further by utilizing Pytorch's `nn.Linear` layers, which effectively perform matrix multiplications when the bias units are disabled."
      ],
      "metadata": {
        "id": "Fcm3wEsuZMzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## self-attention class using pytorch's\n",
        "class SelfAttention_v2(nn.Module):\n",
        " def __init__(self,d_in,d_out,qkv_bias=False):\n",
        "  super().__init__()\n",
        "  self.w_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "  self.w_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "  self.w_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "\n",
        " def forward(self,x):\n",
        "   keys  = self.w_key(x)\n",
        "   queries = self.w_query(x)\n",
        "   values = self.w_value(x)\n",
        "   attn_scores = queries @ keys.T\n",
        "   attn_weights = torch.softmax(\n",
        "      attn_scores/keys.shape[-1]**0.5,dim=-1\n",
        "  )\n",
        "   context_vec = attn_weights @ values\n",
        "   return context_vec"
      ],
      "metadata": {
        "id": "yC83DImgY9Jy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##use case\n",
        "torch.manual_seed(123)\n",
        "sa_v2 = SelfAttention_v2(d_in,d_out)\n",
        "context_vector = sa_v2(inputs)\n",
        "print(f\"Context Vector:{context_vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf7cm2-TdCd4",
        "outputId": "f7e0acab-bca6-49ae-a8fe-c4d362449abc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector:tensor([[ 0.6918,  0.4175, -0.7206, -0.2338,  0.2625],\n",
            "        [ 0.6856,  0.4112, -0.7157, -0.2336,  0.2580],\n",
            "        [ 0.6935,  0.4199, -0.7217, -0.2341,  0.2648],\n",
            "        [ 0.6856,  0.4111, -0.7156, -0.2336,  0.2579],\n",
            "        [ 0.6878,  0.4134, -0.7174, -0.2337,  0.2595]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.3 Causal Attention"
      ],
      "metadata": {
        "id": "7Zw_NGF_d-kN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* For many LLM tasks, you will want the self-attention mechanism to consider only tokens that appear prior to the current position when predicting the next token in a sequence.\n",
        "* Causal attention restricts a model to only consider previous and current inputs in a sequence when processing any given token when computing attention scores."
      ],
      "metadata": {
        "id": "R00kQ-76eCwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##applying a causal attention\n",
        "#1 computing the attention weights using softmax\n",
        "queries = sa_v2.w_query(inputs)\n",
        "keys = sa_v2.w_key(inputs)\n",
        "attn_scores = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5,dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF51GE9YdCgW",
        "outputId": "c055a922-42e6-49d3-fd6d-535cccc6a627"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2090, 0.1883, 0.2181, 0.1878, 0.1968],\n",
            "        [0.2184, 0.1807, 0.2265, 0.1803, 0.1941],\n",
            "        [0.2103, 0.1907, 0.2120, 0.1908, 0.1962],\n",
            "        [0.2184, 0.1806, 0.2266, 0.1802, 0.1941],\n",
            "        [0.2151, 0.1834, 0.2238, 0.1830, 0.1948]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#masking values along diagonals\n",
        "context_length = attn_scores.shape[0]\n",
        "mask = torch.tril(torch.ones(context_length,context_length))\n",
        "print(mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuHOlw9dgC_q",
        "outputId": "4bff4210-1fab-4a2b-cff9-7f915ca1c943"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#multiplying mask with attention weights\n",
        "masked = attn_weights*mask\n",
        "print(masked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2K-kIhOgd_6",
        "outputId": "4797bb32-8e21-4190-df7b-c845927d6cd0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2090, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2184, 0.1807, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2103, 0.1907, 0.2120, 0.0000, 0.0000],\n",
            "        [0.2184, 0.1806, 0.2266, 0.1802, 0.0000],\n",
            "        [0.2151, 0.1834, 0.2238, 0.1830, 0.1948]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## renormalize the attention weights to sum up to to 1\n",
        "row_sums = masked.sum(dim=1,keepdim=True)\n",
        "masked_norm = masked /row_sums\n",
        "print(masked_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTV3aZKLhU_b",
        "outputId": "b3e33a81-c7e7-4fd5-b137-d0517797a256"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5472, 0.4528, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3431, 0.3111, 0.3458, 0.0000, 0.0000],\n",
            "        [0.2710, 0.2242, 0.2812, 0.2236, 0.0000],\n",
            "        [0.2151, 0.1834, 0.2238, 0.1830, 0.1948]], grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The softmax function converts its inputs into a probability distribution.\n",
        "* When negative infinity values are present in a row, the softmax function treats them as zero probability."
      ],
      "metadata": {
        "id": "iEliaKsKi2P1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(),-torch.inf)\n",
        "print(masked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17Z_M7mbjyOK",
        "outputId": "44f4a859-379f-4c6f-d2a1-797c146e9c52"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.3166,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-0.5420, -0.9652,    -inf,    -inf,    -inf],\n",
            "        [-0.3316, -0.5503, -0.3142,    -inf,    -inf],\n",
            "        [-0.5402, -0.9646, -0.4576, -0.9698,    -inf],\n",
            "        [-0.4812, -0.8374, -0.3923, -0.8418, -0.7029]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##applying softmax\n",
        "attn_weights = torch.softmax(masked,dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHHSmlyqkVwU",
        "outputId": "d7cf496e-8842-4764-a025-e954e67079cf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.6042, 0.3958, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3545, 0.2848, 0.3607, 0.0000, 0.0000],\n",
            "        [0.2949, 0.1929, 0.3203, 0.1919, 0.0000],\n",
            "        [0.2330, 0.1632, 0.2547, 0.1625, 0.1867]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.3.1 Masking additional weights with dropout"
      ],
      "metadata": {
        "id": "MvNUkBdYsfgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `Dropout` in deep learning is a technique where randomly selected hidden layer units are ignored during training, effectively `dropping` the out.\n",
        "* This method helps prevent overfitting by ensuring that a model does not become overly reliant on any specific set of hidden layer units.\n",
        "* In the transformer architecture, dropout in the attention mechanism is typically applied at two specific times: after calculating the attention weights or after applying the attention weights to the value vectors."
      ],
      "metadata": {
        "id": "N59x7IisstVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##example\n",
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5)#dropout of 50%\n",
        "example = torch.ones(6,6)\n",
        "print(dropout(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83Yt4OZlt67T",
        "outputId": "7ef7e3b6-88fb-4f61-e418-24926dd04833"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 0., 2., 2., 0.],\n",
            "        [0., 0., 0., 2., 0., 2.],\n",
            "        [2., 2., 2., 2., 0., 2.],\n",
            "        [0., 2., 2., 0., 0., 2.],\n",
            "        [0., 2., 0., 2., 0., 2.],\n",
            "        [0., 2., 2., 2., 2., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* When applying dropout to an attention weight matrix with a rate of 50%, half of the elements in the matrix are randomly set to zero.\n",
        "* To compensate for the reduction in active elements,the values of the reamining elements in the matrix are scaled up by a factor of 1/0.5 = 2.\n",
        "* This scaling is crucial to maintain the overall balance of the attention weights, ensuring that the average influence of attention mechanism remains consistent during both the training and inference phases."
      ],
      "metadata": {
        "id": "lOEKq3JSuVOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##applying dropout to the attention weight matrix\n",
        "torch.manual_seed(123)\n",
        "print(dropout(attn_weights))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJuRgm9Kvq46",
        "outputId": "e3f7789a-22af-4de0-a430-31532b8361c1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.5697, 0.7214, 0.0000, 0.0000],\n",
            "        [0.5898, 0.0000, 0.6406, 0.0000, 0.0000],\n",
            "        [0.4660, 0.0000, 0.0000, 0.3249, 0.0000]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.3.2 Implementing a compact causal attention class"
      ],
      "metadata": {
        "id": "2S4w8_Ouv7Di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ensuring code can handle batches consisting of more than one input so that\n",
        "##causal attention class supports batch outputs produced by dataloader\n",
        "batch = torch.stack((inputs,inputs),dim=0)\n",
        "print(batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0u_EJx9wL7G",
        "outputId": "074a4484-46e9-4beb-f534-b30dada3fd2a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 5, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The result in a three dimensional tensor consisting of two input texts with 5 tokens each,where each token is a three dimensional embedding vector"
      ],
      "metadata": {
        "id": "WgnAKmURwj1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout_rate,qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.w_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.w_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.w_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.register_buffer(\n",
        "        \"mask\",\n",
        "        torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,num_tokens,d_in = x.shape\n",
        "    keys = self.w_key(x)\n",
        "    queries = self.w_query(x)\n",
        "    values = self.w_value(x)\n",
        "    attn_scores = queries @ keys.transpose(1,2)#tranposing dimensions 1 and 2 keeping the batch dimension at the first position\n",
        "    # Ensure mask is broadcastable or correctly sized for batch operation\n",
        "    attn_scores = attn_scores.masked_fill(self.mask.bool()[:num_tokens,:num_tokens],-torch.inf) # Adjusted mask slicing for batch\n",
        "    attn_weights = torch.softmax(\n",
        "        attn_scores /keys.shape[-1]**0.5, dim=-1\n",
        "    )\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "zsvmC4_dwAQP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in, d_out, qkv_bias=False)\n",
        "context_vecs = ca(batch)\n",
        "print(f\"Context vectors shape:{context_vecs.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXlC_Nre0EaA",
        "outputId": "371f3b85-eb93-43d8-f5c6-d076592aec09"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context vectors shape:torch.Size([2, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The resulting context vector is a three dimensioal tensor where each token is now represented by a five-dimensional embedding"
      ],
      "metadata": {
        "id": "N0P0CHNN1UiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.4 Multi-Head attention"
      ],
      "metadata": {
        "id": "HjCBZr7x1vlB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Our final step is to extended previously implemented causal attention class over multiple heads.\n",
        "* This is called `multi-head attention`.\n",
        "* The terms `multi-head` refers to dividing the attention mechanism into multiple \"heads\" each operating independently."
      ],
      "metadata": {
        "id": "7HrQ_84z13UF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.4.1 Stacking multiple single-head attention layers\n"
      ],
      "metadata": {
        "id": "4bT7gKCp2nk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Implementing multi-head attention involves creating multiple instances of self-attention mechanism, each with it's own weights, and the combining their outputs."
      ],
      "metadata": {
        "id": "ubvDVof_21kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## wrapper class for multi-head attention\n",
        "class MultiHeadAttentionWrappper(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.attn_heads = nn.ModuleList(\n",
        "        [CausalAttention(d_in,d_out,context_length,dropout,qkv_bias) for _ in range(num_heads)]\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return torch.cat([head(x) for head in self.attn_heads],dim=-1)"
      ],
      "metadata": {
        "id": "PIPPVziM3XLa"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "d_in,d_out = 3,2\n",
        "mha = MultiHeadAttentionWrappper(\n",
        "    d_in,d_out,context_length,0.0,num_heads=2\n",
        ")\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wgh_7UK4fTk",
        "outputId": "e5fbeb35-29a7-4d65-d710-6683c54042d5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4992, -0.0313,  0.4853,  0.2945],\n",
            "         [-0.7145, -0.1641,  0.6931,  0.4656],\n",
            "         [-0.6476, -0.0726,  0.6398,  0.3718],\n",
            "         [-0.7172, -0.1349,  0.7042,  0.4430],\n",
            "         [-0.7294, -0.1260,  0.7166,  0.4458]],\n",
            "\n",
            "        [[-0.4992, -0.0313,  0.4853,  0.2945],\n",
            "         [-0.7145, -0.1641,  0.6931,  0.4656],\n",
            "         [-0.6476, -0.0726,  0.6398,  0.3718],\n",
            "         [-0.7172, -0.1349,  0.7042,  0.4430],\n",
            "         [-0.7294, -0.1260,  0.7166,  0.4458]]], grad_fn=<CatBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 5, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The first dimension of the resulting context_vecs tensor is 2 since we have two input texts.\n",
        "* The second dimension refers to 5 tokens in each input.\n",
        "* The third dimension refers to the four-dimensional dimensional embedding of each token."
      ],
      "metadata": {
        "id": "CvBTunpb8_h4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.4.2 Implementing multi-head attention with weight splits"
      ],
      "metadata": {
        "id": "3Xu5HblW7q8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* So far we've implemented a `MultiHeadAttentionWrapper` which performs multi-head attention by stacking multiple single-head attention modules.\n",
        "* In the `MultiHeadAttentionWrapper` class, multiple heads are implemented by creating a list of CausalAttention bojects, each representing a seperate attention head.\n",
        "* Now let's write code which will implement multi-head attention in a single class"
      ],
      "metadata": {
        "id": "OzLMRgQN70Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads\n",
        "    self.w_query = nn.Linear(d_in,d_out,qkv_bias)\n",
        "    self.w_key = nn.Linear(d_in,d_out,qkv_bias)\n",
        "    self.w_value = nn.Linear(d_in,d_out,qkv_bias)\n",
        "    self.out_proj = nn.Linear(d_out,d_out) # Corrected d_in to d_out\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\n",
        "        \"mask\",\n",
        "        torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,num_tokens,d_in = x.shape\n",
        "    keys = self.w_key(x)\n",
        "    values = self.w_value(x)\n",
        "    queries = self.w_query(x)\n",
        "    ##splitting matrix by adding a num_heads and head_dim dimensions\n",
        "    keys= keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    values = values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    queries = queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "\n",
        "    #tranpsoing from shape (b,num_tokens,num_heads,head_dim) to (b,num_heads,num_tokens,head_dim)\n",
        "    keys = keys.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "    queries = queries.transpose(1,2)\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(2,3) # Fixed typo tranpose -> transpose\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens].unsqueeze(0).unsqueeze(0) # Added unsqueeze for broadcasting\n",
        "\n",
        "    attn_scores.masked_fill_(mask_bool,-torch.inf)\n",
        "\n",
        "    attn_weights = torch.softmax(\n",
        "        attn_scores /keys.shape[-1]**0.5,dim=-1\n",
        "    )\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    context_vec_per_head = (attn_weights @ values).transpose(1,2) # Fixed typo tranpose -> transpose and renamed variable\n",
        "\n",
        "    context_vec = context_vec_per_head.contiguous().view( # Used corrected variable name\n",
        "        b,num_tokens,self.d_out\n",
        "    )\n",
        "    context_vec = self.out_proj(context_vec)\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "bq1tXee683bs"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The splitting of the query,key and value tensors is achieved through tensor reshaping and tranposing operations using Pytorch's `.view` and `.transpose` methods.\n",
        "* The key operation is to split the `d_out` dimension into `num_heads` and `head_dim`, where `head_dim=d_out/num_heads`.\n",
        "* This splitting is then achieved using `.view` method: a tensor of dimensions, `(b,num_tokens,d_out)` is reshaped to dimension `(b,num_tokens,num_heada,head_dim)`.\n",
        "* The tensors are then transposed to bring the `num_heads` dimension before the `num_tokens` dimension, resulting in a shape of `(b,num_heads,num_tokens,head_dim)`.\n",
        "* This tranposition is crucial for correctly aligning the queries,keys and values across different heads and performing batched matrix multiplications efficiently.\n"
      ],
      "metadata": {
        "id": "-xY50i-IBsAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##example\n",
        "a = torch.randn(1,2,3,4)\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfACAAA8EJSY",
        "outputId": "e2b17e52-2a40-4471-e356-e0fb903d58a2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-2.2150, -1.3193, -2.0915,  0.9629],\n",
            "          [-0.0319, -0.4790,  0.7668,  0.0275],\n",
            "          [-0.5872,  1.1952, -1.2096, -0.5560]],\n",
            "\n",
            "         [[-2.7202,  0.5421, -1.1541,  0.7763],\n",
            "          [-0.7067, -0.9222,  3.8954, -0.6027],\n",
            "          [-0.0480,  0.5349,  1.1031,  1.3334]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##performing batched matrix multiplication between the tensor itself and view of the tensor where we\n",
        "## transposed the last 2 dimensions , num_tokens ad head_dim\n",
        "print(a @ a.transpose(2,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRaju4O5EZ5L",
        "outputId": "9860e0da-ad00-477f-9918-5ee1ad06f809"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[11.9482, -0.8749,  1.7182],\n",
            "          [-0.8749,  0.8192, -1.4965],\n",
            "          [ 1.7182, -1.4965,  3.5454]],\n",
            "\n",
            "         [[ 9.6278, -3.5411,  0.1825],\n",
            "          [-3.5411, 16.8871,  3.0340],\n",
            "          [ 0.1825,  3.0340,  3.2833]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  In this case, the matrix multiplication implementation in PyTorch handles the four\n",
        "dimensional input tensor so that the matrix multiplication is carried out between the two\n",
        " last dimensions (num_tokens, head_dim) and then repeated for the individual heads."
      ],
      "metadata": {
        "id": "LxDSTMYFFcaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_head = a[0,0,:,:]\n",
        "first_res = first_head @ first_head.T\n",
        "print(\"First head:\\n\",first_res)\n",
        "\n",
        "second_head = a[0,1,:,:]\n",
        "second_res =second_head @ second_head.T\n",
        "print(\"\\nSecond head:\\n\",second_res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTLnaNEgFoyU",
        "outputId": "85e842ee-1667-48ac-90c6-fd6813031fc8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First head:\n",
            " tensor([[11.9482, -0.8749,  1.7182],\n",
            "        [-0.8749,  0.8192, -1.4965],\n",
            "        [ 1.7182, -1.4965,  3.5454]])\n",
            "\n",
            "Second head:\n",
            " tensor([[ 9.6278, -3.5411,  0.1825],\n",
            "        [-3.5411, 16.8871,  3.0340],\n",
            "        [ 0.1825,  3.0340,  3.2833]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Continuing with MultiHeadAttention, after computing the attention weights and con\n",
        "text vectors, the context vectors from all heads are transposed back to the shape `(b,\n",
        " num_tokens, num_heads, head_dim)`. These vectors are then reshaped (flattened) into\n",
        " the shape `(b, num_tokens, d_out)`, effectively combining the outputs from all heads."
      ],
      "metadata": {
        "id": "DNqCbOCjGnUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "batch_size,context_length,d_in = batch.shape\n",
        "d_out = 2\n",
        "mha = MultiHeadAttention(d_in,d_out,context_length,0.0,num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\",context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JAmvxkgHKhv",
        "outputId": "a6bd5d5f-4c26-4887-8633-9056a4d503f5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.2695, 0.4288],\n",
            "         [0.2774, 0.3024],\n",
            "         [0.2874, 0.3482],\n",
            "         [0.2852, 0.3052],\n",
            "         [0.2888, 0.3003]],\n",
            "\n",
            "        [[0.2695, 0.4288],\n",
            "         [0.2774, 0.3024],\n",
            "         [0.2874, 0.3482],\n",
            "         [0.2852, 0.3052],\n",
            "         [0.2888, 0.3003]]], grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 5, 2])\n"
          ]
        }
      ]
    }
  ]
}