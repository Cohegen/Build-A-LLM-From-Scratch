{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPsPJ49R3OboOT//GjxBxmg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s_LNYZcCJP0a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##reusing the multiheadattention class\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads\n",
        "    self.w_query = nn.Linear(d_in,d_out,qkv_bias)\n",
        "    self.w_key = nn.Linear(d_in,d_out,qkv_bias)\n",
        "    self.w_value = nn.Linear(d_in,d_out,qkv_bias)\n",
        "    self.out_proj = nn.Linear(d_out,d_out) # Corrected d_in to d_out\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\n",
        "        \"mask\",\n",
        "        torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,num_tokens,d_in = x.shape\n",
        "    keys = self.w_key(x)\n",
        "    values = self.w_value(x)\n",
        "    queries = self.w_query(x)\n",
        "    ##splitting matrix by adding a num_heads and head_dim dimensions\n",
        "    keys= keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    values = values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    queries = queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "\n",
        "    #tranpsoing from shape (b,num_tokens,num_heads,head_dim) to (b,num_heads,num_tokens,head_dim)\n",
        "    keys = keys.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "    queries = queries.transpose(1,2)\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(2,3) # Fixed typo tranpose -> transpose\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens].unsqueeze(0).unsqueeze(0) # Added unsqueeze for broadcasting\n",
        "\n",
        "    attn_scores.masked_fill_(mask_bool,-torch.inf)\n",
        "\n",
        "    attn_weights = torch.softmax(\n",
        "        attn_scores /keys.shape[-1]**0.5,dim=-1\n",
        "    )\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    context_vec_per_head = (attn_weights @ values).transpose(1,2) # Fixed typo tranpose -> transpose and renamed variable\n",
        "\n",
        "    context_vec = context_vec_per_head.contiguous().view( # Used corrected variable name\n",
        "        b,num_tokens,self.d_out\n",
        "    )\n",
        "    context_vec = self.out_proj(context_vec)\n",
        "    return context_vec\n"
      ],
      "metadata": {
        "id": "GKpBetmZJ7VL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## layernorm class\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean = x.mean(dim=-1,keepdim=True)\n",
        "    var = x.var(dim=-1,keepdim=True)\n",
        "    norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "    return self.scale * norm_x + self.shift\n"
      ],
      "metadata": {
        "id": "nA-cSnb5KzzA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return 0.5 * x * (1 + torch.tanh(\n",
        "        torch.sqrt(torch.tensor(2.0/torch.pi)) * (x + 0.044715 * torch.pow(x,3))\n",
        "    ))\n"
      ],
      "metadata": {
        "id": "SPKQ07wOKuV8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg[\"emb_dim\"],4*cfg[\"emb_dim\"]),\n",
        "        GELU(),\n",
        "        nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)\n"
      ],
      "metadata": {
        "id": "CD-DdgYIKqdz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.att = MultiHeadAttention(\n",
        "        d_in=cfg[\"emb_dim\"],\n",
        "        d_out = cfg[\"emb_dim\"],\n",
        "        context_length = cfg[\"context_size\"],\n",
        "        num_heads = cfg[\"n_heads\"],\n",
        "        dropout = cfg[\"drop_rate\"],\n",
        "        qkv_bias = cfg[\"qkv_bias\"]\n",
        "\n",
        "    )\n",
        "    self.ffn = FeedForward(cfg)\n",
        "    self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "  def forward(self,x):\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.att(x)\n",
        "    x = self.drop_shortcut(x) # Fixed typo: dropout_shortcut -> drop_shortcut\n",
        "    x = x + shortcut\n",
        "\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ffn(x) # Fixed typo: ff -> ffn\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut\n",
        "    return x"
      ],
      "metadata": {
        "id": "7i1J0EuMKhPd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_size\"],cfg[\"emb_dim\"]) # Changed context_length to context_size\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "        *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "\n",
        "    )\n",
        "    self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(\n",
        "        cfg[\"emb_dim\"], cfg[\"vocab_size\"],bias=False\n",
        "    )\n",
        "\n",
        "  def forward(self,in_idx):\n",
        "    batch_size,seq_len = in_idx.shape\n",
        "    tok_embeds= self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(\n",
        "        torch.arange(seq_len,device=in_idx.device)\n",
        "    )\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x) # Corrected dropout application\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "dWG0sqSRKa8U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from IPython.utils.process import get_output_error_code\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\":50257,\n",
        "    \"context_size\":256,\n",
        "    \"emb_dim\":768,\n",
        "    \"n_heads\":12,\n",
        "    \"n_layers\":12,\n",
        "    \"drop_rate\":0.1,\n",
        "    \"qkv_bias\":False\n",
        "}\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgvm52_9LEJP",
        "outputId": "a6460209-dea2-4e5e-bf92-12d06eaa8460"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here we have made some few adjustment to the `GPT_CONFIG_124M` dictionary by reducing the context size to 256 tokens.\n",
        "* This modification reduces the computatioal demands of training the model, making it possible to carry out the training on a standard laptop cmputer"
      ],
      "metadata": {
        "id": "uAUGgOdEMNKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model,idx,max_new_tokens,context_size):\n",
        "  for _ in range(max_new_tokens):\n",
        "    # Crop idx to the last context_size tokens if it's longer\n",
        "    idx_cond = idx[:,  -context_size:]\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)\n",
        "\n",
        "    logits = logits[:,-1,:]\n",
        "    probs = torch.softmax(logits,dim=-1)\n",
        "    idx_next = torch.argmax(probs,dim=-1,keepdim=True)\n",
        "    idx = torch.cat((idx,idx_next),dim=1)\n",
        "  return idx"
      ],
      "metadata": {
        "id": "jN9zmSnnM6W2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##implementing text generation process\n",
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text,tokenizer):\n",
        "  encoded = tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
        "  encoded_tensor = torch.tensor(encoded).unsqueeze(0)# unsqueeze adds the batch dimension\n",
        "  return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids,tokenizer):\n",
        "  flat = token_ids.squeeze(0) #remove batch dimension\n",
        "  return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context,tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_size\"]\n",
        ")\n",
        "print(\"Output text:\\n\",token_ids_to_text(token_ids,tokenizer))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ssZhjQpMrqr",
        "outputId": "24e31387-0e4d-486e-85f4-c4891bdb1a4f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As we can see the model isn't yet producing coherent text because it hasn't undergone training.\n",
        "* To define what makes text 'coherent' or 'high quality' we have to implement a numerical method to evaluate the generated content."
      ],
      "metadata": {
        "id": "WpRLhsHdPJIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2  Calculating the text generation loss."
      ],
      "metadata": {
        "id": "TaXOGNRPPi6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Consider these two input examples, which have already been mapped to token IDs."
      ],
      "metadata": {
        "id": "y7Y9EBmgP0DZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "    [\n",
        "        [16833,3626,6100], #every effort moves\n",
        "        [40,1107,588] # I really like\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "9AMmGbJLP8GB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Matching these inpts, the targets contain token IDs we want the model to produce."
      ],
      "metadata": {
        "id": "KFLXkgWXQPR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "targets = torch.tensor([\n",
        "    [3626,6100,345],#every effort moves you\n",
        "    [1107,588,11311]#really like chocolate\n",
        "])"
      ],
      "metadata": {
        "id": "Yf15rIQpQYJL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Note that these targets are inputs but shifted one position forward. This shifting strategy is crucial for teaching the model to predict the next token in a sequence.\n",
        "* Next we feedthe inputs into the model to calculate logits vectors for the two input examples. The we apply the `softmax` function to transform these logits into probability scores."
      ],
      "metadata": {
        "id": "ZPhXu7hmQs3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  logits = model(inputs)\n",
        "probs = torch.softmax(logits,dim=-1)\n",
        "print(probs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0E6KUARRPo-",
        "outputId": "daeafc9b-2da2-40ef-9248-12b20ffcb4ae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##applying the argmax function to the probability scores to obtain the correspoding token IDs\n",
        "token_ids = torch.argmax(probs,dim=-1,keepdim=True)\n",
        "print(\"Token IDs:\\n\",token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTRqCRH3Rm9r",
        "outputId": "a3c133c9-db93-4853-d4f6-fdd47215190a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[[16657],\n",
            "         [  339],\n",
            "         [42826]],\n",
            "\n",
            "        [[49906],\n",
            "         [29669],\n",
            "         [41751]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Given that we have two input batches, each containing three tokens, applying `argmax` function to the probability scores yields two set of outputs, each with three predicted token Ids"
      ],
      "metadata": {
        "id": "0ZqVYEUTSKia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##converting the token IDs back into text\n",
        "print(f\"Target batch1:{token_ids_to_text(targets[0],tokenizer)}\")\n",
        "print(f\"Outputs batch1:\"f\"{token_ids_to_text(token_ids[0].flatten(),tokenizer)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1Dmi7TGSqif",
        "outputId": "5db6a9d1-f334-4fe7-eba8-3d406abd1479"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target batch1: effort moves you\n",
            "Outputs batch1: Armed heNetflix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As we can see the model produces random text that is different from the target text because it has not been trained yet.\n",
        "* Model training aims to increase the softmax probability in the index postions corresponding to the correct target token Ids.\n",
        "* This softmax probability is also used in evaluation metric we will implement next to numerically access the model's generated outputs: higher probability in the correct positions, the better."
      ],
      "metadata": {
        "id": "7E1cE0-BTpE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_idx = 0\n",
        "target_probs1 = probs[text_idx,[0,1,2],targets[text_idx]]\n",
        "print(\"Text 1:\",target_probs1)\n",
        "\n",
        "text_idx = 1\n",
        "target_probs2 = probs[text_idx,[0,1,2],targets[text_idx]]\n",
        "print(\"Text 2: \",target_probs2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdepar6rUspY",
        "outputId": "c32d104b-3271-441f-ebb0-fa020792cbdd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1: tensor([7.4514e-05, 3.1054e-05, 1.1567e-05])\n",
            "Text 2:  tensor([1.0343e-05, 5.6737e-05, 4.7620e-06])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The goal of training an LLM is to maximize relative likelihood of the current token, which involves increasing its probability relative to other tokens.\n",
        "* This way, we ensure the LLM consistently picks the target token i.e the next word in the sentence as the next tokem it generates."
      ],
      "metadata": {
        "id": "ffUd2NVEVudy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##calculating probability scores of the two example batches\n",
        "log_probs =torch.log(torch.cat((target_probs1,target_probs2)))\n",
        "print(log_probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmsYrII2YexF",
        "outputId": "954b3108-9e76-4e24-c2eb-e31f85ce311b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ -9.5045, -10.3798, -11.3674, -11.4792,  -9.7771, -12.2549])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Working with logarithms of the probabilities scores is more managable in mathematical optimization than handling the scores directly."
      ],
      "metadata": {
        "id": "KnEVgEgVY31j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##combining the log probabilities into  single score by computing the averagr\n",
        "avg_log_probs = torch.mean(log_probs)\n",
        "print(avg_log_probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYXLEG1ZZLsu",
        "outputId": "13ed7277-1579-447a-e2e5-f12e46978e5c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-10.7938)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The goal is to get the average log probability as close to 0 as possilbe by updating the model's weights as part of the training process.\n",
        "* However, in deep learning, the common practice isn't to push the average log probability up to 0 but rather to bring the negative average log probability to 0.\n",
        "* The negative average log probability is simply the average log probability multiplied by -1 ."
      ],
      "metadata": {
        "id": "LU5NL63JZhd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neg_avg_log_probs = avg_log_probs *-1\n",
        "print(neg_avg_log_probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2Niy_Voawj-",
        "outputId": "966438df-8519-4110-c1cf-7e9ab431a4df"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.7938)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#recalling shape of the logits and target tensors\n",
        "print(\"Logits shape: \",logits.shape) #dim=batch_size,num_tokens,vocab_size\n",
        "print(\"Target shape: \",targets.shape)#batch_size and num_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMgt3mJpa9xT",
        "outputId": "4f7235b6-7626-4b8e-cba9-a8c91cd3d5bf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape:  torch.Size([2, 3, 50257])\n",
            "Target shape:  torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##flattening tensors by combining them over the batch dimension\n",
        "logits_flat = logits.flatten(0,1)\n",
        "targets_flat = targets.flatten()\n",
        "print(\"Flattened logits: \",logits_flat.shape)\n",
        "print(\"Flattened targets: \",targets_flat.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buwOolK_bsAw",
        "outputId": "24281b0f-a09f-4b38-9060-20690e17df5b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flattened logits:  torch.Size([6, 50257])\n",
            "Flattened targets:  torch.Size([6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Previously, we applied the softmax function, selected the probability scores corresponding to the target IDs, and computed the negative average log probabilites.\n",
        "* Pytorch's `cross_entropy` function will take care of the these steps."
      ],
      "metadata": {
        "id": "O7qGZkEBcvxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss= torch.nn.functional.cross_entropy(logits_flat,targets_flat)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfSAl1LAcP2t",
        "outputId": "b0ec4538-6503-4aad-b321-048588485929"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.7938)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The above resulting loss is the same that we obtained previously when applying the individual steps manually."
      ],
      "metadata": {
        "id": "VOhXHcoXclcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Calculating the trainig and validation set losses."
      ],
      "metadata": {
        "id": "wlgoSnbLdUzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We must first prepare the training and validation datasets that we will use to train the LLM.\n",
        "* To compute loss, we use a very samll text dataset, the \"Notes from the underground\""
      ],
      "metadata": {
        "id": "Yy1fJzqfdcwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "url = \"https://www.gutenberg.org/cache/epub/600/pg600.txt\"\n",
        "input_file =\"pg600.txt\"\n",
        "urllib.request.urlretrieve(url, input_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9WpObyteHVP",
        "outputId": "391c044f-a6b7-432e-cd61-bc488e40390d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('pg600.txt', <http.client.HTTPMessage at 0x7d550d631910>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(input_file,'r',encoding='utf-8') as f:\n",
        "  text_data = f.read()"
      ],
      "metadata": {
        "id": "iAmVAuYueetU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "print(\"Characters: \",total_characters)\n",
        "print(\"Tokens:\",total_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-mdOEm5epz3",
        "outputId": "49b8920b-59d2-4d33-e679-0a89a7817b43"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters:  259118\n",
            "Tokens: 67308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##diving data into training and validation set\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio*len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]"
      ],
      "metadata": {
        "id": "9DFMXu2FfQsP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using the `train_data` and `val_data` subsets, we can now create the respective dataloader reusing the `create_dataloader_v1` function"
      ],
      "metadata": {
        "id": "buXyzObpfwwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## creating a dataset for batched inputs and targets\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self,txt,tokenizer,max_length,stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(txt)##tokenizes text\n",
        "\n",
        "    for i in range(0,len(token_ids) - max_length,stride):#using sliding window to chuk the book into overlapping sequences of max_length\n",
        "      input_chunk = token_ids[i:i+max_length]\n",
        "      target_chunk = token_ids[i+1:i+max_length+1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "TNuw7nwLiYcD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## creating a dataloader to generate batches with input-with pairs\n",
        "def create_dataloader_v1(txt,batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      drop_last=drop_last,\n",
        "      num_workers=num_workers\n",
        "  )\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "kHZLJ1aif_xd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_size\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_size\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_size\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_size\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "Oc14ZANae1NO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train loader\")\n",
        "for x,y in train_loader:\n",
        "  print(x.shape,y.shape)\n",
        "print(\"\\nValidation loader:\")\n",
        "for x,y in val_loader:\n",
        "  print(x.shape,y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ljeRL2kihoP",
        "outputId": "f1a4db97-3e28-4f92-ee3b-190ad608412f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "Validation loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##implementing a utility function to calculate the cross entropy loss\n",
        "## of a given batch returned via the training and validation loader\n",
        "def calc_loss_batch(input_batch,target_batch,model,device):\n",
        "  input_batch = input_batch.to(device)\n",
        "  target_batch = target_batch.to(device)\n",
        "  logits = model(input_batch)\n",
        "  loss = torch.nn.functional.cross_entropy(\n",
        "      logits.flatten(0,1),target_batch.flatten()\n",
        "  )\n",
        "  return loss"
      ],
      "metadata": {
        "id": "5WhNMjN0i7sw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##implementing a function to compute the training and validation loss\n",
        "def calc_loss_loader(data_loader,model,device,num_batches=None):\n",
        "  total_loss = 0\n",
        "  if len(data_loader) == 0:\n",
        "    return float(\"nan\")\n",
        "\n",
        "  # Determine the actual number of batches to process\n",
        "  if num_batches is None:\n",
        "    actual_num_batches = len(data_loader)\n",
        "  else:\n",
        "    actual_num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "  for i, (input_batch,target_batch) in enumerate(data_loader):\n",
        "    if i < actual_num_batches:\n",
        "      loss = calc_loss_batch(\n",
        "          input_batch,target_batch,model,device\n",
        "      )\n",
        "      total_loss += loss.item()\n",
        "    else:\n",
        "      break\n",
        "  # Avoid division by zero if actual_num_batches is 0\n",
        "  if actual_num_batches == 0:\n",
        "    return float(\"nan\")\n",
        "  return total_loss / actual_num_batches ##averaging loss over all batches\n"
      ],
      "metadata": {
        "id": "ZDgKO6aWjqr-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_loader,model,device)\n",
        "  val_loss = calc_loss_loader(val_loader,model,device)\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\",val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w04IVeiomF2X",
        "outputId": "2eacdc13-0532-46b1-de13-df4cb2264582"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 10.979752308180352\n",
            "Validation loss: 10.972686608632406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The loss values are relatively high because the model has not yet been trainied."
      ],
      "metadata": {
        "id": "D9KQp8INnrSY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Training the LLM to generate human like text\n"
      ],
      "metadata": {
        "id": "QIh4sbQmn5T8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,train_loader,val_loader,optimizer,device,num_epochs,eval_freq,eval_iter,start_context,tokenizer):\n",
        "  train_losses,val_losses,track_tokens_seen = [], [], []\n",
        "  tokens_seen,global_step = 0, -1\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for input_batch,target_batch in train_loader:\n",
        "      optimizer.zero_grad() #rest loss gradients from previous batch iterations\n",
        "      loss = calc_loss_batch(\n",
        "          input_batch,target_batch,model,device\n",
        "      )\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      tokens_seen += input_batch.numel()\n",
        "      global_step += 1\n",
        "\n",
        "      if global_step % eval_freq == 0:\n",
        "        train_loss,val_loss = evaluate_model(\n",
        "            model,train_loader,val_loader,eval_iter\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        track_tokens_seen.append(tokens_seen)\n",
        "        print(\n",
        "            f\"Ep {epoch+1} (Step {global_step:06d}):\"\n",
        "            f\"Train Loss {train_loss:.3f},\"\n",
        "            f\"Val loss {val_loss:.2f}\"\n",
        "        )\n",
        "    generate(\n",
        "       model,tokenizer,device,start_context\n",
        "   )\n",
        "  return train_losses,val_losses,track_tokens_seen"
      ],
      "metadata": {
        "id": "mpxhSebboG-U"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model,train_loader,val_loader,eval_iter):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(\n",
        "        train_loader,model,device,num_batches=eval_iter\n",
        "    )\n",
        "    val_loss = calc_loss_loader(\n",
        "        val_loader,model,device,num_batches=eval_iter\n",
        "    )\n",
        "  model.train()\n",
        "  return train_loss,val_loss"
      ],
      "metadata": {
        "id": "6pCPHeTeq_Ju"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model,tokenizer,device,start_context):\n",
        "  model.eval()\n",
        "  context_size = model.pos_emb.weight.shape[0]\n",
        "  encoded = text_to_token_ids(start_context,tokenizer).to(device)\n",
        "  with torch.no_grad():\n",
        "    token_ids = generate_text(\n",
        "        model=model,\n",
        "        idx=encoded,\n",
        "        max_new_tokens=50,\n",
        "        context_size=context_size\n",
        "    )\n",
        "  decoded_text =token_ids_to_text(token_ids,tokenizer)\n",
        "  print(decoded_text.replace(\"\\n\",\" \"))\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "iPZ_nSYcrEjk"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=0.0004,weight_decay=0.1\n",
        ")\n",
        "num_epochs = 10\n",
        "train_losses,val_losses,tokens_seen = train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    num_epochs=num_epochs,\n",
        "    eval_freq=5,\n",
        "    eval_iter=5,\n",
        "    start_context=\"I am a sick\",\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuUQTyx-jayV",
        "outputId": "0b8fed94-2f51-4df7-c147-bc637aeaee24"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000):Train Loss 9.905,Val loss 9.86\n",
            "Ep 1 (Step 000005):Train Loss 8.366,Val loss 8.42\n",
            "Ep 1 (Step 000010):Train Loss 7.244,Val loss 7.64\n",
            "Ep 1 (Step 000015):Train Loss 6.927,Val loss 7.39\n",
            "Ep 1 (Step 000020):Train Loss 6.880,Val loss 7.43\n",
            "Ep 1 (Step 000025):Train Loss 6.651,Val loss 7.41\n",
            "Ep 1 (Step 000030):Train Loss 6.837,Val loss 7.35\n",
            "Ep 1 (Step 000035):Train Loss 6.775,Val loss 7.20\n",
            "Ep 1 (Step 000040):Train Loss 6.496,Val loss 7.17\n",
            "Ep 1 (Step 000045):Train Loss 6.331,Val loss 7.08\n",
            "Ep 1 (Step 000050):Train Loss 6.154,Val loss 7.02\n",
            "Ep 1 (Step 000055):Train Loss 5.990,Val loss 6.99\n",
            "Ep 1 (Step 000060):Train Loss 6.182,Val loss 6.94\n",
            "Ep 1 (Step 000065):Train Loss 6.158,Val loss 6.89\n",
            "Ep 1 (Step 000070):Train Loss 6.055,Val loss 6.85\n",
            "Ep 1 (Step 000075):Train Loss 5.944,Val loss 6.81\n",
            "Ep 1 (Step 000080):Train Loss 6.000,Val loss 6.78\n",
            "Ep 1 (Step 000085):Train Loss 5.729,Val loss 6.80\n",
            "Ep 1 (Step 000090):Train Loss 6.010,Val loss 6.67\n",
            "Ep 1 (Step 000095):Train Loss 5.633,Val loss 6.63\n",
            "Ep 1 (Step 000100):Train Loss 6.000,Val loss 6.64\n",
            "Ep 1 (Step 000105):Train Loss 5.870,Val loss 6.57\n",
            "Ep 1 (Step 000110):Train Loss 5.669,Val loss 6.55\n",
            "Ep 1 (Step 000115):Train Loss 5.682,Val loss 6.51\n",
            "I am a sick        â€ â€ â€ â€ â€ â€ â€ â€ â€ â€ â€ â€ â€ â€ \n",
            "Ep 2 (Step 000120):Train Loss 5.541,Val loss 6.51\n",
            "Ep 2 (Step 000125):Train Loss 5.637,Val loss 6.52\n",
            "Ep 2 (Step 000130):Train Loss 5.638,Val loss 6.57\n",
            "Ep 2 (Step 000135):Train Loss 5.544,Val loss 6.53\n",
            "Ep 2 (Step 000140):Train Loss 5.890,Val loss 6.50\n",
            "Ep 2 (Step 000145):Train Loss 5.361,Val loss 6.49\n",
            "Ep 2 (Step 000150):Train Loss 5.523,Val loss 6.53\n",
            "Ep 2 (Step 000155):Train Loss 5.291,Val loss 6.51\n",
            "Ep 2 (Step 000160):Train Loss 5.198,Val loss 6.56\n",
            "Ep 2 (Step 000165):Train Loss 5.205,Val loss 6.49\n",
            "Ep 2 (Step 000170):Train Loss 5.351,Val loss 6.50\n",
            "Ep 2 (Step 000175):Train Loss 5.385,Val loss 6.50\n",
            "Ep 2 (Step 000180):Train Loss 5.228,Val loss 6.47\n",
            "Ep 2 (Step 000185):Train Loss 5.290,Val loss 6.53\n",
            "Ep 2 (Step 000190):Train Loss 5.423,Val loss 6.47\n",
            "Ep 2 (Step 000195):Train Loss 5.343,Val loss 6.50\n",
            "Ep 2 (Step 000200):Train Loss 5.211,Val loss 6.47\n",
            "Ep 2 (Step 000205):Train Loss 5.285,Val loss 6.45\n",
            "Ep 2 (Step 000210):Train Loss 5.087,Val loss 6.51\n",
            "Ep 2 (Step 000215):Train Loss 5.477,Val loss 6.43\n",
            "Ep 2 (Step 000220):Train Loss 5.148,Val loss 6.41\n",
            "Ep 2 (Step 000225):Train Loss 5.196,Val loss 6.42\n",
            "Ep 2 (Step 000230):Train Loss 5.271,Val loss 6.35\n",
            "Ep 2 (Step 000235):Train Loss 5.032,Val loss 6.37\n",
            "I am a sick, to, and to to the room, and the room, and to to the room, and the room, and the room, and to the room, and the room, and to the room, and the\n",
            "Ep 3 (Step 000240):Train Loss 5.327,Val loss 6.31\n",
            "Ep 3 (Step 000245):Train Loss 5.195,Val loss 6.37\n",
            "Ep 3 (Step 000250):Train Loss 5.134,Val loss 6.31\n",
            "Ep 3 (Step 000255):Train Loss 5.122,Val loss 6.34\n",
            "Ep 3 (Step 000260):Train Loss 5.007,Val loss 6.39\n",
            "Ep 3 (Step 000265):Train Loss 5.174,Val loss 6.41\n",
            "Ep 3 (Step 000270):Train Loss 4.954,Val loss 6.36\n",
            "Ep 3 (Step 000275):Train Loss 5.047,Val loss 6.39\n",
            "Ep 3 (Step 000280):Train Loss 4.911,Val loss 6.35\n",
            "Ep 3 (Step 000285):Train Loss 5.104,Val loss 6.31\n",
            "Ep 3 (Step 000290):Train Loss 4.778,Val loss 6.31\n",
            "Ep 3 (Step 000295):Train Loss 4.773,Val loss 6.32\n",
            "Ep 3 (Step 000300):Train Loss 4.849,Val loss 6.37\n",
            "Ep 3 (Step 000305):Train Loss 4.998,Val loss 6.35\n",
            "Ep 3 (Step 000310):Train Loss 4.917,Val loss 6.32\n",
            "Ep 3 (Step 000315):Train Loss 4.959,Val loss 6.36\n",
            "Ep 3 (Step 000320):Train Loss 4.785,Val loss 6.39\n",
            "Ep 3 (Step 000325):Train Loss 4.856,Val loss 6.30\n",
            "Ep 3 (Step 000330):Train Loss 4.783,Val loss 6.30\n",
            "Ep 3 (Step 000335):Train Loss 4.670,Val loss 6.32\n",
            "Ep 3 (Step 000340):Train Loss 4.444,Val loss 6.29\n",
            "Ep 3 (Step 000345):Train Loss 4.734,Val loss 6.30\n",
            "Ep 3 (Step 000350):Train Loss 4.666,Val loss 6.30\n",
            "Ep 3 (Step 000355):Train Loss 4.964,Val loss 6.36\n",
            "I am a sick. I am                                               \n",
            "Ep 4 (Step 000360):Train Loss 4.648,Val loss 6.35\n",
            "Ep 4 (Step 000365):Train Loss 4.739,Val loss 6.32\n",
            "Ep 4 (Step 000370):Train Loss 4.689,Val loss 6.37\n",
            "Ep 4 (Step 000375):Train Loss 4.710,Val loss 6.40\n",
            "Ep 4 (Step 000380):Train Loss 4.524,Val loss 6.41\n",
            "Ep 4 (Step 000385):Train Loss 4.720,Val loss 6.37\n",
            "Ep 4 (Step 000390):Train Loss 4.713,Val loss 6.35\n",
            "Ep 4 (Step 000395):Train Loss 4.598,Val loss 6.35\n",
            "Ep 4 (Step 000400):Train Loss 4.666,Val loss 6.35\n",
            "Ep 4 (Step 000405):Train Loss 4.689,Val loss 6.39\n",
            "Ep 4 (Step 000410):Train Loss 4.759,Val loss 6.38\n",
            "Ep 4 (Step 000415):Train Loss 4.825,Val loss 6.38\n",
            "Ep 4 (Step 000420):Train Loss 4.536,Val loss 6.39\n",
            "Ep 4 (Step 000425):Train Loss 4.668,Val loss 6.37\n",
            "Ep 4 (Step 000430):Train Loss 4.377,Val loss 6.38\n",
            "Ep 4 (Step 000435):Train Loss 4.467,Val loss 6.41\n",
            "Ep 4 (Step 000440):Train Loss 4.492,Val loss 6.35\n",
            "Ep 4 (Step 000445):Train Loss 4.567,Val loss 6.32\n",
            "Ep 4 (Step 000450):Train Loss 4.614,Val loss 6.28\n",
            "Ep 4 (Step 000455):Train Loss 4.390,Val loss 6.32\n",
            "Ep 4 (Step 000460):Train Loss 4.279,Val loss 6.31\n",
            "Ep 4 (Step 000465):Train Loss 4.496,Val loss 6.28\n",
            "Ep 4 (Step 000470):Train Loss 4.295,Val loss 6.28\n",
            "Ep 4 (Step 000475):Train Loss 4.295,Val loss 6.32\n",
            "I am a sick, and, and, and                   â€œWhy,â€ I was in the contrary, that I was not the contrary, I was in the contrary,\n",
            "Ep 5 (Step 000480):Train Loss 4.176,Val loss 6.29\n",
            "Ep 5 (Step 000485):Train Loss 4.271,Val loss 6.30\n",
            "Ep 5 (Step 000490):Train Loss 3.999,Val loss 6.29\n",
            "Ep 5 (Step 000495):Train Loss 4.133,Val loss 6.34\n",
            "Ep 5 (Step 000500):Train Loss 4.005,Val loss 6.33\n",
            "Ep 5 (Step 000505):Train Loss 3.889,Val loss 6.31\n",
            "Ep 5 (Step 000510):Train Loss 4.010,Val loss 6.32\n",
            "Ep 5 (Step 000515):Train Loss 3.978,Val loss 6.36\n",
            "Ep 5 (Step 000520):Train Loss 3.988,Val loss 6.35\n",
            "Ep 5 (Step 000525):Train Loss 3.742,Val loss 6.40\n",
            "Ep 5 (Step 000530):Train Loss 4.170,Val loss 6.45\n",
            "Ep 5 (Step 000535):Train Loss 3.960,Val loss 6.43\n",
            "Ep 5 (Step 000540):Train Loss 4.229,Val loss 6.43\n",
            "Ep 5 (Step 000545):Train Loss 3.793,Val loss 6.37\n",
            "Ep 5 (Step 000550):Train Loss 3.874,Val loss 6.35\n",
            "Ep 5 (Step 000555):Train Loss 3.943,Val loss 6.41\n",
            "Ep 5 (Step 000560):Train Loss 4.033,Val loss 6.38\n",
            "Ep 5 (Step 000565):Train Loss 3.768,Val loss 6.42\n",
            "Ep 5 (Step 000570):Train Loss 3.931,Val loss 6.39\n",
            "Ep 5 (Step 000575):Train Loss 3.883,Val loss 6.36\n",
            "Ep 5 (Step 000580):Train Loss 3.813,Val loss 6.39\n",
            "Ep 5 (Step 000585):Train Loss 3.701,Val loss 6.35\n",
            "Ep 5 (Step 000590):Train Loss 3.523,Val loss 6.36\n",
            "I am a sick â€œI will â€ I am,â€œI am not to me to â€œI am a year you will â€ â€œOh,â€œOh,â€œYou will â€œWhy,ï¿½\n",
            "Ep 6 (Step 000595):Train Loss 3.746,Val loss 6.33\n",
            "Ep 6 (Step 000600):Train Loss 3.806,Val loss 6.38\n",
            "Ep 6 (Step 000605):Train Loss 3.405,Val loss 6.41\n",
            "Ep 6 (Step 000610):Train Loss 3.468,Val loss 6.40\n",
            "Ep 6 (Step 000615):Train Loss 3.495,Val loss 6.42\n",
            "Ep 6 (Step 000620):Train Loss 3.318,Val loss 6.43\n",
            "Ep 6 (Step 000625):Train Loss 3.654,Val loss 6.39\n",
            "Ep 6 (Step 000630):Train Loss 3.435,Val loss 6.44\n",
            "Ep 6 (Step 000635):Train Loss 3.490,Val loss 6.48\n",
            "Ep 6 (Step 000640):Train Loss 3.262,Val loss 6.46\n",
            "Ep 6 (Step 000645):Train Loss 3.338,Val loss 6.44\n",
            "Ep 6 (Step 000650):Train Loss 3.461,Val loss 6.47\n",
            "Ep 6 (Step 000655):Train Loss 3.552,Val loss 6.48\n",
            "Ep 6 (Step 000660):Train Loss 3.284,Val loss 6.52\n",
            "Ep 6 (Step 000665):Train Loss 3.063,Val loss 6.51\n",
            "Ep 6 (Step 000670):Train Loss 3.092,Val loss 6.51\n",
            "Ep 6 (Step 000675):Train Loss 3.444,Val loss 6.47\n",
            "Ep 6 (Step 000680):Train Loss 3.225,Val loss 6.52\n",
            "Ep 6 (Step 000685):Train Loss 2.962,Val loss 6.44\n",
            "Ep 6 (Step 000690):Train Loss 3.098,Val loss 6.48\n",
            "Ep 6 (Step 000695):Train Loss 3.262,Val loss 6.48\n",
            "Ep 6 (Step 000700):Train Loss 3.094,Val loss 6.47\n",
            "Ep 6 (Step 000705):Train Loss 3.100,Val loss 6.51\n",
            "Ep 6 (Step 000710):Train Loss 3.104,Val loss 6.45\n",
            "I am a sickly, in the same time I was and that that I was in the all this very different of the and, in the and so I was so on the same time to be reconciled to be reconciled to see, but\n",
            "Ep 7 (Step 000715):Train Loss 2.959,Val loss 6.47\n",
            "Ep 7 (Step 000720):Train Loss 3.091,Val loss 6.51\n",
            "Ep 7 (Step 000725):Train Loss 2.863,Val loss 6.56\n",
            "Ep 7 (Step 000730):Train Loss 2.905,Val loss 6.55\n",
            "Ep 7 (Step 000735):Train Loss 2.707,Val loss 6.57\n",
            "Ep 7 (Step 000740):Train Loss 2.816,Val loss 6.58\n",
            "Ep 7 (Step 000745):Train Loss 2.589,Val loss 6.62\n",
            "Ep 7 (Step 000750):Train Loss 2.592,Val loss 6.59\n",
            "Ep 7 (Step 000755):Train Loss 2.587,Val loss 6.63\n",
            "Ep 7 (Step 000760):Train Loss 2.564,Val loss 6.63\n",
            "Ep 7 (Step 000765):Train Loss 2.850,Val loss 6.61\n",
            "Ep 7 (Step 000770):Train Loss 3.083,Val loss 6.57\n",
            "Ep 7 (Step 000775):Train Loss 2.579,Val loss 6.60\n",
            "Ep 7 (Step 000780):Train Loss 2.356,Val loss 6.62\n",
            "Ep 7 (Step 000785):Train Loss 2.610,Val loss 6.65\n",
            "Ep 7 (Step 000790):Train Loss 2.434,Val loss 6.63\n",
            "Ep 7 (Step 000795):Train Loss 2.400,Val loss 6.64\n",
            "Ep 7 (Step 000800):Train Loss 2.649,Val loss 6.66\n",
            "Ep 7 (Step 000805):Train Loss 2.608,Val loss 6.66\n",
            "Ep 7 (Step 000810):Train Loss 2.266,Val loss 6.64\n",
            "Ep 7 (Step 000815):Train Loss 2.228,Val loss 6.63\n",
            "Ep 7 (Step 000820):Train Loss 2.255,Val loss 6.64\n",
            "Ep 7 (Step 000825):Train Loss 2.269,Val loss 6.70\n",
            "Ep 7 (Step 000830):Train Loss 2.295,Val loss 6.64\n",
            "I am a sickble, and that it was so on I was a sick man? Why are still in the same time the same I was a man, and with the same. heve to be in the same time to be But as\n",
            "Ep 8 (Step 000835):Train Loss 2.276,Val loss 6.66\n",
            "Ep 8 (Step 000840):Train Loss 1.977,Val loss 6.68\n",
            "Ep 8 (Step 000845):Train Loss 2.257,Val loss 6.73\n",
            "Ep 8 (Step 000850):Train Loss 1.927,Val loss 6.76\n",
            "Ep 8 (Step 000855):Train Loss 2.136,Val loss 6.81\n",
            "Ep 8 (Step 000860):Train Loss 1.787,Val loss 6.80\n",
            "Ep 8 (Step 000865):Train Loss 2.055,Val loss 6.80\n",
            "Ep 8 (Step 000870):Train Loss 1.930,Val loss 6.83\n",
            "Ep 8 (Step 000875):Train Loss 1.854,Val loss 6.87\n",
            "Ep 8 (Step 000880):Train Loss 1.657,Val loss 6.83\n",
            "Ep 8 (Step 000885):Train Loss 1.861,Val loss 6.90\n",
            "Ep 8 (Step 000890):Train Loss 1.574,Val loss 6.90\n",
            "Ep 8 (Step 000895):Train Loss 2.038,Val loss 6.84\n",
            "Ep 8 (Step 000900):Train Loss 2.021,Val loss 6.93\n",
            "Ep 8 (Step 000905):Train Loss 1.877,Val loss 6.88\n",
            "Ep 8 (Step 000910):Train Loss 1.897,Val loss 6.89\n",
            "Ep 8 (Step 000915):Train Loss 1.553,Val loss 6.88\n",
            "Ep 8 (Step 000920):Train Loss 1.603,Val loss 6.92\n",
            "Ep 8 (Step 000925):Train Loss 1.608,Val loss 6.95\n",
            "Ep 8 (Step 000930):Train Loss 1.751,Val loss 6.97\n",
            "Ep 8 (Step 000935):Train Loss 1.817,Val loss 6.94\n",
            "Ep 8 (Step 000940):Train Loss 1.392,Val loss 6.96\n",
            "Ep 8 (Step 000945):Train Loss 1.640,Val loss 6.95\n",
            "Ep 8 (Step 000950):Train Loss 1.468,Val loss 6.98\n",
            "I am a sickly so I am a girl on at and that my head of course, I should have a for my face, and so. But now, I should have been that I should go. I should have been positively surprised at my duty to\n",
            "Ep 9 (Step 000955):Train Loss 1.609,Val loss 7.00\n",
            "Ep 9 (Step 000960):Train Loss 1.325,Val loss 7.05\n",
            "Ep 9 (Step 000965):Train Loss 1.253,Val loss 7.04\n",
            "Ep 9 (Step 000970):Train Loss 1.464,Val loss 7.07\n",
            "Ep 9 (Step 000975):Train Loss 1.019,Val loss 7.16\n",
            "Ep 9 (Step 000980):Train Loss 1.237,Val loss 7.17\n",
            "Ep 9 (Step 000985):Train Loss 1.147,Val loss 7.16\n",
            "Ep 9 (Step 000990):Train Loss 1.422,Val loss 7.16\n",
            "Ep 9 (Step 000995):Train Loss 1.168,Val loss 7.19\n",
            "Ep 9 (Step 001000):Train Loss 1.053,Val loss 7.19\n",
            "Ep 9 (Step 001005):Train Loss 1.289,Val loss 7.18\n",
            "Ep 9 (Step 001010):Train Loss 1.305,Val loss 7.21\n",
            "Ep 9 (Step 001015):Train Loss 0.840,Val loss 7.15\n",
            "Ep 9 (Step 001020):Train Loss 1.228,Val loss 7.18\n",
            "Ep 9 (Step 001025):Train Loss 0.891,Val loss 7.22\n",
            "Ep 9 (Step 001030):Train Loss 1.014,Val loss 7.26\n",
            "Ep 9 (Step 001035):Train Loss 1.241,Val loss 7.28\n",
            "Ep 9 (Step 001040):Train Loss 0.902,Val loss 7.20\n",
            "Ep 9 (Step 001045):Train Loss 1.108,Val loss 7.27\n",
            "Ep 9 (Step 001050):Train Loss 1.023,Val loss 7.26\n",
            "Ep 9 (Step 001055):Train Loss 1.089,Val loss 7.26\n",
            "Ep 9 (Step 001060):Train Loss 0.799,Val loss 7.27\n",
            "Ep 9 (Step 001065):Train Loss 0.933,Val loss 7.22\n",
            "Ep 9 (Step 001070):Train Loss 0.932,Val loss 7.29\n",
            "I am a sick-room, where there was only one candle burning, and stood still more advantageousently a good?â€ â€™s ready to be,â€ she answered,â€™s. â€™s a slave!â€œYes\n",
            "Ep 10 (Step 001075):Train Loss 0.904,Val loss 7.32\n",
            "Ep 10 (Step 001080):Train Loss 0.918,Val loss 7.37\n",
            "Ep 10 (Step 001085):Train Loss 0.763,Val loss 7.38\n",
            "Ep 10 (Step 001090):Train Loss 0.666,Val loss 7.40\n",
            "Ep 10 (Step 001095):Train Loss 0.748,Val loss 7.40\n",
            "Ep 10 (Step 001100):Train Loss 0.660,Val loss 7.44\n",
            "Ep 10 (Step 001105):Train Loss 0.638,Val loss 7.48\n",
            "Ep 10 (Step 001110):Train Loss 0.794,Val loss 7.49\n",
            "Ep 10 (Step 001115):Train Loss 0.733,Val loss 7.44\n",
            "Ep 10 (Step 001120):Train Loss 0.684,Val loss 7.48\n",
            "Ep 10 (Step 001125):Train Loss 0.638,Val loss 7.49\n",
            "Ep 10 (Step 001130):Train Loss 0.603,Val loss 7.48\n",
            "Ep 10 (Step 001135):Train Loss 0.661,Val loss 7.52\n",
            "Ep 10 (Step 001140):Train Loss 0.603,Val loss 7.52\n",
            "Ep 10 (Step 001145):Train Loss 0.601,Val loss 7.55\n",
            "Ep 10 (Step 001150):Train Loss 0.660,Val loss 7.54\n",
            "Ep 10 (Step 001155):Train Loss 0.611,Val loss 7.56\n",
            "Ep 10 (Step 001160):Train Loss 0.560,Val loss 7.60\n",
            "Ep 10 (Step 001165):Train Loss 0.461,Val loss 7.59\n",
            "Ep 10 (Step 001170):Train Loss 0.476,Val loss 7.55\n",
            "Ep 10 (Step 001175):Train Loss 0.415,Val loss 7.58\n",
            "Ep 10 (Step 001180):Train Loss 0.691,Val loss 7.55\n",
            "Ep 10 (Step 001185):Train Loss 0.497,Val loss 7.58\n",
            "I am a sick-gown round me out of sheer? But that distant relation another three months; and good luck on the  for the most other times I was in my desires, for a they had put myself publicly on an equal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##plotting curves\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "def plot_losses(epochs_seen,tokens_seen,train_losses,val_losses):\n",
        "  fig,ax1 = plt.subplots(figsize=(5,3))\n",
        "  ax1.plot(epochs_seen,train_losses,label=\"Training Loss\")\n",
        "  ax1.plot(epochs_seen,val_losses,linestyle=\"-.\",label=\"Validation loss\")\n",
        "\n",
        "  ax1.set_xlabel(\"Epochs\")\n",
        "  ax1.set_ylabel(\"Loss\")\n",
        "  ax1.legend(loc=\"upper right\")\n",
        "  ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "  ax2 = ax1.twiny()\n",
        "  ax2.plot(tokens_seen,train_losses,alpha=0)\n",
        "  ax2.set_xlabel(\"Tokens seen\")\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0,num_epochs,len(train_losses))\n",
        "plot_losses(epochs_tensor,tokens_seen,train_losses,val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "Y1n260GSs3Ta",
        "outputId": "2c335299-d07c-4375-a60f-33118d38c409"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}