{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMMOgNRnEu1YUQlomXqO8b+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Introduction to instruction fine-tuning."
      ],
      "metadata": {
        "id": "UyNsX4UCqD6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here, we focus on improving  the LLM's ability to follow instructions and generate a desired response.\n",
        "* This will involve three strategies i.e dataset preparation,fine-tuning the LLM and evaluating the LLM."
      ],
      "metadata": {
        "id": "avxrmtnDqI8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Preparing a datset for supervised instruction fine-tuning."
      ],
      "metadata": {
        "id": "pCBqiV6LqlXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json,os,urllib\n",
        "\n",
        "def download_load_file(file_path,url):\n",
        "  if not os.path.exists(file_path):\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "      text_data = response.read().decode('utf-8')\n",
        "    with open(file_path,\"w\",encoding='utf-8') as file:\n",
        "      file.write(text_data)\n",
        "\n",
        "  with open(file_path,'r') as file:\n",
        "    data = json.load(file)\n",
        "  return data\n",
        "\n",
        "file_path = \"instruction-data.json\"\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
        "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        ")\n",
        "\n",
        "data = download_load_file(file_path, url)\n",
        "print(\"Number of entries: \",len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeKazZrPqsTo",
        "outputId": "eb9815ef-92de-4476-cb73-c9f74461105d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries:  1100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##printing data list from the json file\n",
        "print(\"Dample entry:\\n\",data[50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJYgz1IIr0Yn",
        "outputId": "a361f999-818b-490e-9a4a-09d73e628539"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dample entry:\n",
            " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Instruction fine-tuning involves training a model on a dataset where the input-output pairs, like those we extracted from the JSON file, are provided."
      ],
      "metadata": {
        "id": "p9i3GUbIsOW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##implementing a prompt function\n",
        "def format_input(entry):\n",
        "  instruction_text = (\n",
        "      f\"Below is an instruction that describes a task. \"\n",
        "      f\"Write a response that appropriately completes the request. \"\n",
        "      f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "\n",
        "  )\n",
        "  input_text = (\n",
        "      f\"\\n\\n## Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "  )\n",
        "  return instruction_text + input_text"
      ],
      "metadata": {
        "id": "ulxRRXZkskjr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This `format_input` function takes a dictionary `entry` as input and constructs a formatted string."
      ],
      "metadata": {
        "id": "gqCMvFdvuber"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = format_input(data[50])\n",
        "desired_response = f\"\\n\\n### Response: \\n{data[50]['output']}\"\n",
        "print(model_input + desired_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MR6gElXFunEz",
        "outputId": "06871d26-65aa-48c5-e5d9-1c7a762cb5a7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
            "\n",
            "### Instruction:\n",
            "Identify the correct spelling of the following word.\n",
            "\n",
            "## Input:\n",
            "Ocassion\n",
            "\n",
            "### Response: \n",
            "The correct spelling is 'Occasion.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = format_input(data[999])\n",
        "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
        "print(model_input + desired_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07BSWyvGv_-8",
        "outputId": "fc573276-4807-4073-9ea6-fcfe7e127f33"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
            "\n",
            "### Instruction:\n",
            "What is an antonym of 'complicated'?\n",
            "\n",
            "### Response:\n",
            "An antonym of 'complicated' is 'simple'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##dividing dataset into training,validation and test sets\n",
        "train_portion = int(len(data)* 0.85)\n",
        "test_portion = int(len(data)*0.1)\n",
        "val_portion = len(data) - train_portion - test_portion\n",
        "\n",
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:train_portion+test_portion]\n",
        "val_data = data[train_portion + test_portion:]\n",
        "\n",
        "print(\"Training set length:\",len(train_data))\n",
        "print(\"Validation set length:\",len(val_data))\n",
        "print(\"Test set length:\",len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW7ZbIn3wv9P",
        "outputId": "1c16bb9c-52d9-4149-b251-7118f052778a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set length: 935\n",
            "Validation set length: 55\n",
            "Test set length: 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Organizing data into training batches"
      ],
      "metadata": {
        "id": "_vjCmBeRyKfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In the previous chapter, the training batches were created automatically by the PyTorch\n",
        "DataLoader class, which employs a default collate function to combine lists of samples\n",
        "into batches.\n",
        "* A collate function is responsible for taking a list of individual data sam\n",
        "ples and merging them into a single batch that can be processed efficiently by the\n",
        "model during training.\n",
        "* However, the batching process for instruction fine-tuning is a bit more involved\n",
        "and requires us to create our own custom collate function that we will later plug into the DataLoader.\n",
        "* We implement this custom collate function to handle the specific\n",
        "requirements and formatting of our instruction fine-tuning dataset.  \n"
      ],
      "metadata": {
        "id": "MczbQAU1ywt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##implementing an instruction dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "  def __init__(self,data,tokenizer):\n",
        "    self.data = data\n",
        "    self.encoded_texts = []\n",
        "    for entry in data:\n",
        "      instruction_input = format_input(entry)\n",
        "      response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "      full_text = instruction_input + response_text\n",
        "      self.encoded_texts.append(\n",
        "          tokenizer.encode(full_text)\n",
        "      )\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.encoded_texts[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "metadata": {
        "id": "ufm4IoazyuXe"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "## adding endoftext token whose id will be appended\n",
        "## to the pretokenized input directly\n",
        "print(tokenizer.encode(\"<|endoftext|>\",allowed_special={\"<|endoftext|>\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCpxk7yl0H5D",
        "outputId": "7fece032-1b61-4e35-9187-37304c232193"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50256]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## defining a colliate function that pads the training\n",
        "## examples in each batch to have different lengths\n",
        "def custom_collate1(batch,pad_token_id=50256,device='cpu'):\n",
        "  batch_max_length = max(len(item) + 1 for item in batch)\n",
        "  inputs_lst = []\n",
        "\n",
        "  for item in batch:\n",
        "    new_item = item.copy()\n",
        "    new_item += [pad_token_id]\n",
        "\n",
        "    padded = (\n",
        "        new_item + [pad_token_id] *(batch_max_length - len(new_item))\n",
        "    )\n",
        "    inputs = torch.tensor(padded[:-1])\n",
        "    inputs_lst.append(inputs)\n",
        "\n",
        "  input_tensors = torch.stack(inputs_lst).to(device)\n",
        "  return input_tensors"
      ],
      "metadata": {
        "id": "zknZqVAk02Kw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## example case\n",
        "input_1 = [0,1,2,3,4]\n",
        "input_2 = [5,6]\n",
        "input_3 = [7,8,9]\n",
        "batch = (\n",
        "    input_1,\n",
        "    input_2,\n",
        "    input_3\n",
        ")\n",
        "print(custom_collate1(batch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3qqVHZI2Wyp",
        "outputId": "54ca1c9e-3323-44fd-d780-5ec076803496"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This output shows all inputs have been padded to the lenth of the longest input list `input_1`, which contains five token IDs."
      ],
      "metadata": {
        "id": "XMPOg1qw3fKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## creating a colliate function for target token Ids\n",
        "def custom_collate2(batch,pad_token_id=50256,device=\"cpu\"):\n",
        "  batch_max_length = max(len(item)+1 for item in batch)\n",
        "  input_lst,targets_lst = [],[]\n",
        "\n",
        "  for item in batch:\n",
        "    new_item = item.copy()\n",
        "    new_item += [pad_token_id]\n",
        "    padded = (\n",
        "        new_item + [pad_token_id]*\n",
        "        (batch_max_length) - len(new_item)\n",
        "    )\n",
        "    inputs = torch.tensor(padded[:-1])\n",
        "    targets = torch.tensor(padded[1:])\n",
        "    input_lst.apped(inputs)\n",
        "    targets_lst.append(targets)\n",
        "  inputs,targets = custom_collate2(batch)\n",
        "  print(inputs)\n",
        "  print(targets)\n",
        "\n"
      ],
      "metadata": {
        "id": "D-7NLFbH39gb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##implementing a full custom batch collate function\n",
        "def custom_collate(batch,pad_token_id=50256,ignore_index=-100,allowed_max_length=None,device=\"cpu\"):\n",
        "  batch_max_length = max(len(item)+1 for item in batch)\n",
        "  inputs_lst,targets_lst = [],[]\n",
        "\n",
        "  for item in batch:\n",
        "    new_item = item.copy()\n",
        "    new_item += [pad_token_id]\n",
        "\n",
        "    padded = (\n",
        "        new_item + [pad_token_id]*(batch_max_length - len(new_item))\n",
        "    )\n",
        "    inputs = torch.tensor(padded[:-1])#truncates the last token for inputs\n",
        "    targets = torch.tensor(padded[1:])# shits +1 to the right for targets\n",
        "\n",
        "\n",
        "\n",
        "    ##replaces all but the first padding tokens in targets by ignore_index\n",
        "    mask = targets == pad_token_id\n",
        "    indices = torch.nonzero(mask).squeeze()\n",
        "    if indices.numel() > 1:\n",
        "      targets[indices[1:]] = ignore_index\n",
        "\n",
        "\n",
        "\n",
        "    if allowed_max_length is not None:\n",
        "      inputs = inputs[:allowed_max_length]\n",
        "      targets = targets[:allowed_max_length]\n",
        "\n",
        "    inputs_lst.append(inputs)\n",
        "    targets_lst.append(targets)\n",
        "\n",
        "  inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "  targets_tensor = torch.stack(targets_lst).to(device)\n",
        "  return inputs_tensor,targets_tensor"
      ],
      "metadata": {
        "id": "wwfrU4Xu6FTX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##use case\n",
        "inputs,targets = custom_collate(batch)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVioRs7T8oiD",
        "outputId": "0a2c6125-e217-4c50-bd03-e6527f54585c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256,  -100,  -100,  -100],\n",
            "        [    8,     9, 50256,  -100,  -100]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits_1 = torch.tensor(\n",
        "    [[-1.0,1.0],[-0.5,1.5]]\n",
        ")\n",
        "targets_1 = torch.tensor([0,1]) #correct tensors to generate\n",
        "loss_1 = torch.nn.functional.cross_entropy(logits_1,targets_1)\n",
        "print(loss_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW-O7kBc89uJ",
        "outputId": "c859ee88-7901-4531-c4c3-2feb2812ea87"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1269)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits_2 = torch.tensor(\n",
        "    [[-1.0, 1.0],\n",
        "     [-0.5, 1.5],\n",
        "     [-0.5, 1.5]]\n",
        ")\n",
        "targets_2 = torch.tensor([0, 1, 1])\n",
        "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
        "print(loss_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozX2eAS39Z9H",
        "outputId": "14b64bba-40ae-48bb-98ba-d91b55a53714"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7936)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##seeing what happens if we replace the third target token\n",
        "## with -100\n",
        "targets_3 = torch.tensor([0,1,-100])\n",
        "loss_3 = torch.nn.functional.cross_entropy(logits_2,targets_3)\n",
        "print(loss_3)\n",
        "print(\"loss_1 == loss_3: \",loss_1== loss_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrXPInja9jBP",
        "outputId": "472ecb76-acb2-4a6f-fffc-a354f33c825b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1269)\n",
            "loss_1 == loss_3:  tensor(True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Creating data loader for an instruction dataset"
      ],
      "metadata": {
        "id": "5FZ8EJxQ-yI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##intializing device variable\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.backends.mps.is_available():\n",
        "  device = torch.device(\"mps\")\n",
        "print(\"Device:\",device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRJaVrjS_kOV",
        "outputId": "ad23087a-3f94-44bf-c5bc-3c1f3c39bb03"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## reusing the chosen device setting in custom_collate function\n",
        "## using 'partial' function to create a new version of the function\n",
        "## from Pytho's argument prefilled\n",
        "from functools import partial\n",
        "\n",
        "customized_collate_fn = partial(\n",
        "    custom_collate,\n",
        "    device=device,\n",
        "    allowed_max_length=1024\n",
        ")"
      ],
      "metadata": {
        "id": "flh91HeEAB-G"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##initializing the data loaders\n",
        "from torch.utils.data import DataLoader\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset = InstructionDataset(train_data,tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        "    collate_fn = custom_collate\n",
        ")\n",
        "val_dataset = InstructionDataset(val_data,tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        "    collate_fn = custom_collate\n",
        ")\n",
        "test_dataset = InstructionDataset(test_data,tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers = num_workers,\n",
        "    collate_fn = custom_collate\n",
        ")"
      ],
      "metadata": {
        "id": "ORwA_Q_8As0C"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}